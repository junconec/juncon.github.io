<!DOCTYPE html>
<html lang="zh-CN">





<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/e.jpg">
  <link rel="icon" type="image/png" href="/img/j.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content>
  <meta name="author" content="Juncon">
  <meta name="keywords" content>
  <title>JUNCON个人博客</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css">
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/github-v2.min.css">

<link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css">


  



</head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>JUNCON个人博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/rom.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: scroll;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fa fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-md">
        <div class="py-5 z-depth-3" id="board">
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                


  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/05/03/scrapy-5-文件图片下载/">
        <p class="h4 index-header">scrapy-5-文件图片下载</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">在scrapy中提供了2种下载通道
FilesPipeline    提供文件下载
ImagesPipline  提供图片的下载，额外提供缩略图
这2个ItemPipline作为特殊的下载器，用户使用时，只需要通过item的一个特殊字段，将要下载文件或图片url进行赋值，他们就会自动将文件或者图片下载本地。并将下载结果信息存入到item的另一个特殊字段中
FilePipline的使用
导入路径  scrapy.pipelines.files.FilesPipeline
Item字段  file_urls,files
下载目录  FILES_STORE
请看一下前端页面
12345678&lt;html&gt; &lt;body&gt; &lt;a href='/book/sg.pdf'&gt;下载三国演义&lt;/a&gt; &lt;a href='/book/shz.pdf'&gt;下载水浒传&lt;/a&gt; &lt;a href='/book/hlm.pdf'&gt;下载红楼梦&lt;/a&gt; &lt;a href='/book/xyj.pdf'&gt;下载西游记&lt;/</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-05-03&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/categories/scrapy-5-%E6%96%87%E4%BB%B6%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD">scrapy-5-文件图片下载</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/tags/scrapy">scrapy</a>&nbsp;
          
            <a href="/tags/scrapy-5-%E6%96%87%E4%BB%B6%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD">scrapy-5-文件图片下载</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/04/25/docker/">
        <p class="h4 index-header">docker</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">####Python操作docker
通过python调用docker，需要和docker在同一个服务器上才可以
也可以通过传递
1unix:///var/run1/docker.sock

来进行在容器内操作你指定想要连接的docker，这个共享需要在容器最开始创建时候，让容器和外部一个文件进行关联才可以，具体可以参照一下
docker run -d -it -p 805:80 -p 5002:5000 -p 10027:22 -p 20001:8888 -p 20002:3306 –name centos_master –privileged=true -v /var/run/docker.sock:/var/run/docker.sock -v /zc:/zc -e LANG=zh_CN.utf8 zhangcheng0111/centos7-webssh-dockermanager-ssh /usr/sbin/init
其中比较关键的是-v /var/run/docker.sock:/var/run/docker.sock  映射了容器内部和外部的关联文件
以下介绍通过pyth</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-04-25&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/bug">bug</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/docker">docker</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/04/05/scrapy-4-链接提取器/">
        <p class="h4 index-header">scrapy-4-链接提取器</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">链接提取器，设置规则后，进行批量提取所有连接，主要用于是在CrawlSpider模板中使用的居多
比如提取某个网站上所有数据
使用LinkExtractor需要添加相关头文件
1from scrapy.linkextractors import LinkExtractor

以下代码进行整站爬取。通过Rule+LinkExtractor结合进行提取了所有页面的数据，CrawlSpider想进行整站所有a标签提取，但是Rule+LinkExtractor进行了限制，筛选出符合我规则的数据
1234567891011import scrapyfrom scrapy.spiders import *from scrapy.linkextractors import LinkExtractorclass ItcastSpider(scrapy.spiders.CrawlSpider):     name = 'book'     start_urls = ['http://books.toscrape.com/']     img_urls = []     rules = (Rule(Lin</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-04-05&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/categories/scrapy-4-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8">scrapy-4-链接提取器</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/tags/scrapy">scrapy</a>&nbsp;
          
            <a href="/tags/scrapy-4-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8">scrapy-4-链接提取器</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/03/25/python/">
        <p class="h4 index-header">Django序列化器问题</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">序列化器
12345678class BookInfoSerializer(serializers.Serializer):        """图书数据序列化器"""        id = serializers.IntegerField(label='ID', read_only=True)        btitle = serializers.CharField(label='名称', max_length=20)        bpub_date = serializers.DateField(label='发布日期', required=False)        bread = serializers.IntegerField(label='阅读量', required=False)        bcomment = serializers.IntegerField(label='评论量', required=False)        image = serializers.ImageField(label='图片', required=False)#     在这</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-03-25&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/python">python</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/Django">Django</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/03/02/scrapy-3-数据处理/">
        <p class="h4 index-header">scrapy-3-数据处理</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">####itemPipeline数据处理
介绍
启动方式
导出数据
一、介绍
在scrapy中，数据经过spider请求解析后，就需要进行对应模型化处理，在scrapy中itemPipeline负责数据处理
   itemPipeline相当于数据处理的过滤器一样，可以配置多个，每一个itemPipline处理完成return后，会进入下一个itemPipeline中，如果不return，则表示放弃这段数据
   每一个itemPipeline中，需要实现process_item(self,item,spider):这个方法，在这个方法中进行处理数据
   open_spider(self,spider) 一般Spider打开时，回调该方法，通常用于处理数据之前的某些初始化工作，比如连接数据库
   close_spider(self,spider)一般Spider关闭时，回调该方法，通常用于处理完成所有数据之后，如关闭数据库
   from_crawler(cls,crawler) 一般没啥用，创建ItemPipeline对象时候回调该类方法
二、启动方式
启动方式有2种方式，一种在</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-03-02&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/categories/scrapy-3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">scrapy-3-数据处理</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/tags/scrapy">scrapy</a>&nbsp;
          
            <a href="/tags/scrapy-3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">scrapy-3-数据处理</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/02/27/scrapy-2-数据解析/">
        <p class="h4 index-header">scrapy-2-数据解析</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1、Request和Respose对象
2、Selector对象
3、xpath使用
4、css选择器
一、Request和Respose对象
1.1 Request对象
Request对象是要请求一个连接所使用的对象
1Request(url[,call,method=‘GET’,headers,body,cookies,meta,encoding=‘utf-8’,priority=0,dont_filter=False,errback])

Url是请求地址
callback是页面回调地址
method是请求方法，get或者post
headers是请求头
body是html正文
cookies字典类型
Meta字典类型
encoding是编码格式
priority优先级 0是最高级
dont_filter=False对重复地址自动过滤，如果设置为True会强制请求，适合重复地址随时间变化的
errback请求错误
1.2 Respose对象
Respose对象是当发起请求后，返回的对象，比如下列代码中parse参数中的response
12345class BookSpider(</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-02-27&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/categories/scrapy-2-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90">scrapy-2-数据解析</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/tags/scrapy">scrapy</a>&nbsp;
          
            <a href="/tags/scrapy-2-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90">scrapy-2-数据解析</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/01/26/docker部署Django/">
        <p class="h4 index-header">docker部署Django</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">一、安装docker
二、拉取镜像
三、运行镜像，映射端口
四、安装容器内运行环境
五、主机与容器间资源传输
六、容器安装+配置 uwsgi
七、容器安装+ 配置nginx
八、负载均衡
九、Docker提交保存镜像
十、坑点
#######################################################################
前言
Django部署流程如下
1、docker安装
2、拉取centos镜像或者Ubuntu镜像  看你用哪个
3、使用镜像，run出来一个容器A
4、进入容器A，安装uwsgi，把Django部署在下面
5、在启动脚本中配置开机自启动脚本(这步有点难，需要命令方面需要使用特权)
6、提交容器A成为新镜像，run这个新镜像为容器，项目就部署完成
6、在使用centos镜像或者Ubuntu镜像，run一个容器B，在容器B中安装nginx，配置nginx转发地址即可完成部署
说明
1、在以后项目迁移到任何操作系统下通吃
2、多并发时候，可以通过run多个容器A,在配置nginx即可完成
##################</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-01-26&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/Django">Django</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/docker">docker</a>&nbsp;
          
            <a href="/tags/Django">Django</a>&nbsp;
          
            <a href="/tags/%E9%83%A8%E7%BD%B2">部署</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2017/01/26/scrapy-1-快速入门/">
        <p class="h4 index-header">scrapy-1-快速入门</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">本教程环境是在Mac下正确安装的，windows下各个操作系统没有测试过
在学习前需要了解scrapy能干什么
1、scrapy的设计全部采用异步请求，能够全面提高爬取抓取速度
2、自动配置了重复请求过滤的操作
3、可以通过命令行参数，默认导出json，csv等格式，Excel需要自己实现
4、通过管道下载文件
注：scrapy创建工程和运行程序，都是需要通过终端来完成的，无法通过可视化界面来创建
一、基本入门
安装
1pip install scrapy

安装有可能会出错，更新一下pip
1pip3 install --upgrade pip

在终端中输入python回车，输入一下内容
1import scrapyscrapy.version_info

以上编译通过不报错即可
创建项目
1scrapy startproject demo

demo是你工程的名字,默认创建在你当前终端所在的路径上
为了方便后续开发，可以使用pycharm打开这个工程
spider文件夹下创建爬虫文件bookspider.py
12345class ItcastSpider(scrapy.spid</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2017-01-26&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/categories/scrapy-1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">scrapy-1-快速入门</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
            <a href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>&nbsp;
          
            <a href="/tags/scrapy">scrapy</a>&nbsp;
          
            <a href="/tags/scrapy-1-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">scrapy-1-快速入门</a>&nbsp;
          
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2016/05/02/Python-打包exe/">
        <p class="h4 index-header">Python-打包exe</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Python-5-打包exe通过这篇文章，你能学习到以下内容
1、使用pyinstaller进行打包
打包exe文件,使用pyinstaller，pyinstaller支持python2和python3
目前存在问题:
1、pyinstaller 打包是在什么环境下打包，可以在什么环境下执行，无法在Mac下打包exe
2、执行后大量占用内存，最后无法执行完成(有待修复)
安装
12pip install pyinstallerpip install --upgrade pyinstaller

将cmd的目录切换至（命令：cd 文件路径(注意空格)）需要打包的py文件目录下：
1pyinstaller -F MyTools.py

参数说明
123456–icon=图标路径-F 打包成一个exe文件-w 使用窗口，无控制台-c 使用控制台，无窗口-D 创建一个目录，里面包含exe以及其他一些依赖性文件pyinstaller -h 来查看参数

回车后，直到操作结束。返回目标文件目录，发现该目录下生成了.spec文件test.spec：打包好的exe文件，在同目录的dist文件中：my.</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2016-05-02&nbsp;&nbsp;
        
        
          <i class="iconfont icon-inbox"></i>
          
            <a href="/categories/python">python</a>&nbsp;
          &nbsp;
        
        
          <i class="iconfont icon-tag"></i>
          
            <a href="/tags/python">python</a>&nbsp;
          
        
      </div>
    </div>
  </div>



  <nav aria-label="index posts navigation">
    <ul class="pagination pg-blue justify-content-center mt-5">
      
        <li class="page-item">
          <a class="page-link" href="/page/2/#board">Previous</a>
        </li>
      
      
    </ul>
  </nav>



              </div>
            </div>
          </div>
        </div>
      </div>
    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>
    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "&nbsp;",
      ],
      cursorChar: "The greatest generosity to the future is to give something",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "true",
      
      icon: "§"
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $('#post').find('img').each(
      function () {
        var _this = $(this);
        var _src = _this.attr("src");
        _this.wrap('<a data-fancybox="images" href="' + _src + '" ></a>');
      }
    );
  </script>




</body>
</html>
