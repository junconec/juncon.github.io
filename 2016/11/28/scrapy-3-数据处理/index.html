<!DOCTYPE html>
<html lang="zh-CN">





<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/e.jpg">
  <link rel="icon" type="image/png" href="/img/l.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content>
  <meta name="author" content="Juncon">
  <meta name="keywords" content>
  <title>scrapy-3-数据处理 ~ juncon的个人博客</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css">
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/github-v2.min.css">

<link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css">


  



</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>juncon的个人博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/r.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: scroll;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              <p class="mt-3">星期一, 十一月 28日 2016, 10:18 上午</p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>####itemPipeline数据处理</p>
<p>介绍</p>
<p>启动方式</p>
<p>导出数据</p>
<p>一、介绍</p>
<p>在scrapy中，数据经过spider请求解析后，就需要进行对应模型化处理，在scrapy中itemPipeline负责数据处理</p>
<p>   itemPipeline相当于数据处理的过滤器一样，可以配置多个，每一个itemPipline处理完成return后，会进入下一个itemPipeline中，如果不return，则表示放弃这段数据</p>
<p>   每一个itemPipeline中，需要实现process_item(self,item,spider):这个方法，在这个方法中进行处理数据</p>
<p>   open_spider(self,spider) 一般Spider打开时，回调该方法，通常用于处理数据之前的某些初始化工作，比如连接数据库</p>
<p>   close_spider(self,spider)一般Spider关闭时，回调该方法，通常用于处理完成所有数据之后，如关闭数据库</p>
<p>   from_crawler(cls,crawler) 一般没啥用，创建ItemPipeline对象时候回调该类方法</p>
<p>二、启动方式</p>
<p>启动方式有2种方式，一种在setting.py进行配置，一种在爬虫文件中配置</p>
<p>方式1 在setting.py中进行配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;<span class="string">'example.pipelines.PriceConverterPipeline'</span>:<span class="number">300</span>&#125;</span><br></pre></td></tr></table></figure>

<p>example是工程名字，pipelines是所在文件夹，PriceConverterPipeline自定义的itemPipeline，300是优先级，数值越小优先度越大</p>
<p>多个ItemPipLine的应用，可以使用去重操作，在自定义的ItemPipLine中init初始化时候，初始一个set集合，在每次响应process_item时候对比name，从set中进行对比即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span>   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>      </span><br><span class="line">        self.book_set=set()   </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span>      </span><br><span class="line">            name=item[<span class="string">'name'</span>]      </span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> self.book_set:      </span><br><span class="line">                <span class="comment">#数据重复不进行返回即可      </span></span><br><span class="line">                self.book_sett.add(name)      </span><br><span class="line">                <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>建立完成DuplicatesPipeline后需要在Setting.py中配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;<span class="string">'example.pipelines,PriceConverterPipeline'</span>:<span class="number">300</span>,<span class="string">'example.pipelines.TestPipeline'</span>:<span class="number">400</span>,&#125;</span><br></pre></td></tr></table></figure>



<p>方式2 在Spider文件中可以指定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Class Spider1(scrapy.Spider):     </span><br><span class="line">    name = <span class="string">'book1'</span>     </span><br><span class="line">    custom_settings = &#123;     <span class="string">'ITEM_PIPELINES'</span>:&#123;<span class="string">'pipelineClass1'</span>: <span class="number">300</span>,<span class="string">'pipelineClass2'</span>: <span class="number">400</span>&#125;,                   &#125;</span><br></pre></td></tr></table></figure>

<p>这样可以解决多个Spider1时候指定ITEM_PIPELINES的问题，可以为每一个Spider指定数据处理的方式</p>
<p>需要注意的是最后一个ItemPipLine处理完成后，会根据命令行来进行处理后输出到哪里</p>
<p>scrapy crawl books -o books.csv</p>
<p>以上这段命令行中的-o books.csv是写入的文件，就是在最后一个ItemPipeline最后return的数据</p>
<p><strong>四、数据导出</strong></p>
<p>scrapy默认导出数据支持5种方式</p>
<p>JSON   JsonItemExporter</p>
<p>JSON lines   JsonLinesItemExporter</p>
<p>CSV  CsvItemExporter</p>
<p>XML  XmlItemExporter</p>
<p>Pickle  PickleItemExporter</p>
<p>Marshal MarsshallItemExporter</p>
<p>前4种是极为常用的文本数据格式，后面2种是Python特有的，scrapy本身没有准备Excel格式数据导出</p>
<p>如何导出？</p>
<p>方式一   命令行参数</p>
<p>-o -t参数指定导出文件路径和导出格式，通常t可以由o进行推测完成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl books -o books.csv</span><br></pre></td></tr></table></figure>

<p>scrapy crawl 是固定的</p>
<p>books是spider中的name</p>
<p>-o是输出的路径</p>
<p>books.csv 没有指定其他路径，就是在当前路径下，没有使用o,但是会根据提供的路径，推测出是csv</p>
<p>以下是指定明确的输出格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl books -t json -o books.data</span><br></pre></td></tr></table></figure>

<p>这种数据导出，是依赖什么来进行完成的？</p>
<p>在配置字典FEED_EXPORTERS中搜索Exporter，FEED_EXPORTERS的内容中搜索Exporter，FEED_EXPORTERS的内容由以下二个字典的内容合并而成</p>
<p>默认配置文件中的FEED_EXPORTERS_BASE</p>
<p>用户配置文件中的FEED_EXPORTERS</p>
<p>前者包含内部支持的数据格式，后者包含用户自定义的导出数据格式，以下是Scrapy源码中定义的FEED_EXPORTERS_BASE，它位于scrapy.settings.default_settings模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORTERS_BASE=&#123;<span class="string">'json'</span>:<span class="string">'scrapy.exporters.JsonItemExporter'</span>,<span class="string">'jsonlines'</span>:<span class="string">'scrapy.exporters.JsonLinesItemExporter'</span>,<span class="string">'jl'</span>:<span class="string">'scrapy.exporters.JsonLinesItemExporter'</span>,<span class="string">'csv'</span>:<span class="string">'scrapy.exporters.CsvItemExporter'</span>,<span class="string">'xml'</span>:<span class="string">'scrapy.exporters.XmlItemExporter'</span>,<span class="string">'marshal'</span>:<span class="string">'scrapy.exporters.MarshalItemExporter'</span>,<span class="string">'pickle'</span>:<span class="string">'scrapy.exporters.PickleItemExporter'</span>,&#125;</span><br></pre></td></tr></table></figure>

<p>在这里严重不建议修改源码，因为程序最终是要配置在服务器上面的，服务器上面的环境修改源码，可就没那么简单了</p>
<p>用户添加新的导出格式，通常是在配置文件setting.py中定义FEED_EXPORTERS，比如导出Excel</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORTERS=&#123;<span class="string">'excel'</span>:<span class="string">'my_project.my_exporters.ExcelItemExporter'</span>&#125;</span><br></pre></td></tr></table></figure>

<p>指定导出文件路径，支持%(name)s和%(time)s这2个特殊变量</p>
<p>%(name)s 会被替换为Spider的名字</p>
<p>%(time)s 会被替换为文件创建时间</p>
<p>假设一个项目爬取的有书籍、游戏信息、新闻3个spider</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl books  -o <span class="string">'/export_data/%(name)s/%(time)s.csv'</span></span><br><span class="line">scrapy crawl games  -o <span class="string">'/export_data/%(name)s/%(time)s.csv'</span></span><br><span class="line">scrapy crawl news  -o <span class="string">'/export_data/%(name)s/%(time)s.csv'</span></span><br></pre></td></tr></table></figure>

<p>以上是爬取的内容，根据书名存放在export_data文件夹下的对应书名目录下，每个目录下根据时间进行保存</p>
<p>方式二 配置文件方式</p>
<p>在setting.py中设置以下参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FEED_URI=<span class="string">'export_data/%(name)s.data'</span>            导出数据的位置</span><br><span class="line">FEED_FORMAT=<span class="string">'csv'</span>                               导出数据的格式</span><br><span class="line">FEED_EXPORT_ENCODING=<span class="string">'gbk'</span>                      导出数据的编码</span><br><span class="line">FEED_EXPORT_FIELDS=[<span class="string">'name'</span>,<span class="string">'author'</span>,<span class="string">'price'</span>]    导出数据的字段</span><br><span class="line">FEED_EXPORTERS=&#123;<span class="string">'excel'</span>:<span class="string">'my_project.my_exporters.ExcelItemExporter'</span>&#125;   新添加导出的格式</span><br></pre></td></tr></table></figure>

<p>如何实现一个自定义的导出的Excel类</p>
<p>1、继承BaseItemExporter</p>
<p>其中有3个方法</p>
<p>export_item(self,item) 负责导出爬取的每一项数据，参数item为一项爬取的数据，每一个子类必须实现该方法</p>
<p>start_exporting(self)   在导出开始时被调用，可在该方法执行某些初始化工作</p>
<p>finish_exporting(self) 在导出完成时候被调用，可在该方法中执行某些清理工作</p>
<p>以下实现一个导出Excel,注意这里使用的是xlwt，这个库最多只能在一个sheet中插入65535行数据，如果要插入更多数据，需要切换为openxl来进行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> BaseItemExporter</span><br><span class="line"> </span><br><span class="line">impoort xlwt</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExcelItemExporter</span><span class="params">(BaseItemExporter)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,file,**kwargs)</span>:</span></span><br><span class="line">       self._configure(kwargs)</span><br><span class="line">       self.file=file</span><br><span class="line">       self.wbook=xlwt.Workbook()</span><br><span class="line">       self.wsheet=self.wbook.add_sheet(<span class="string">'scrapy'</span>)</span><br><span class="line">       self.row=<span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">finish_exporting</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.wbook.save(self.file)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">export_item</span><span class="params">(self,item)</span>:</span></span><br><span class="line">        fields=self._get_serialized_fields(item)</span><br><span class="line">    <span class="keyword">for</span> col,v <span class="keyword">in</span> enumerate(x <span class="keyword">for</span> _,x <span class="keyword">in</span> fields):</span><br><span class="line">        self.wsheet.write(self.row,col,v)</span><br><span class="line"> </span><br><span class="line">    self.row +=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>编写完成后再setting.py中添加这个类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORTERS=&#123;<span class="string">'excel'</span>:<span class="string">'my_project.my_exporters.ExcelItemExporter'</span>&#125;</span><br></pre></td></tr></table></figure>

<p>之后终端命令行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl books -t excel -o books.xls</span><br></pre></td></tr></table></figure>

<p>这样就导出excel了，当然你可以封装一个Excel导出类，这样可以更加通用，在web框架下，爬虫框架下都可以使用</p>

            <hr>
          </div>
          <br>
          <div>
            
              <p>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>
                  &nbsp;
                
                  <a class="hover-with-bg" href="/categories/scrapy-3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">scrapy-3-数据处理</a>
                  &nbsp;
                
              </p>
            
            <p>
              <i class="iconfont icon-tag"></i>
              
                <a class="hover-with-bg" href="/tags/python">python</a>
              
                <a class="hover-with-bg" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
              
                <a class="hover-with-bg" href="/tags/scrapy">scrapy</a>
              
                <a class="hover-with-bg" href="/tags/scrapy-3-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86">scrapy-3-数据处理</a>
              
              <span id="/2016/11/28/scrapy-3-数据处理/" class="visitors leancloud_visitors" data-flag-title="scrapy-3-数据处理">
                <em id="visitors-text" class="post-meta-item-text"></em>
                <i id="visitors-count"></i>
              </span>
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>
    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>


  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>


  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "scrapy-3-数据处理&nbsp;",
      ],
      cursorChar: "The greatest generosity to the future is to give something",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "true",
      
      icon: "§"
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $('#post').find('img').each(
      function () {
        var _this = $(this);
        var _src = _this.attr("src");
        _this.wrap('<a data-fancybox="images" href="' + _src + '" ></a>');
      }
    );
  </script>




</body>
</html>
