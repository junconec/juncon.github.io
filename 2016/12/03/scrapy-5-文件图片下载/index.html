<!DOCTYPE html>
<html lang="zh-CN">





<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/e.jpg">
  <link rel="icon" type="image/png" href="/img/l.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content>
  <meta name="author" content="Juncon">
  <meta name="keywords" content>
  <title>scrapy-5-文件图片下载 ~ juncon的个人博客</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css">
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">


  <link rel="stylesheet" href="/lib/prettify/tomorrow.min.css">

<link rel="stylesheet" href="/css/main.css">


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css">


  



</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>juncon的个人博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/r.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: scroll;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              <p class="mt-3">星期六, 十二月 3日 2016, 3:18 下午</p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>在scrapy中提供了2种下载通道</p>
<p>FilesPipeline    提供文件下载</p>
<p>ImagesPipline  提供图片的下载，额外提供缩略图</p>
<p>这2个ItemPipline作为特殊的下载器，用户使用时，只需要通过item的一个特殊字段，将要下载文件或图片url进行赋值，他们就会自动将文件或者图片下载本地。并将下载结果信息存入到item的另一个特殊字段中</p>
<p>FilePipline的使用</p>
<p>导入路径  scrapy.pipelines.files.FilesPipeline</p>
<p>Item字段  file_urls,files</p>
<p>下载目录  FILES_STORE</p>
<p>请看一下前端页面</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'/book/sg.pdf'</span>&gt;</span>下载三国演义<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'/book/shz.pdf'</span>&gt;</span>下载水浒传<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'/book/hlm.pdf'</span>&gt;</span>下载红楼梦<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'/book/xyj.pdf'</span>&gt;</span>下载西游记<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>使用FilePipLine通常置于其他ItemPipline之前</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;<span class="string">'scrapy.pipelines.files.FilesPipeline'</span>:<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>在配置文件setting.py中使用FILES_STORE指定文件下载目录</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FILES_STORE=<span class="string">'/home/zhangcheng/Download/scrapy'</span></span><br></pre></td></tr></table></figure>

<p>在Spider解析一个包含文件下载链接的页面时，将所有需要下载的文件的url地址收集到一个列表，赋给item的file_urls字段(item[‘file_urls’]) FilesPipeline在处理每一项item时，会读取item[‘file_urls’],对其中每一个url进行下载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DownloadBookSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(response)</span>:</span></span><br><span class="line"> item=&#123;&#125;</span><br><span class="line"> <span class="comment">#下载列表</span></span><br><span class="line"> item[<span class="string">'file_urls'</span>]=[]</span><br><span class="line"> <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line"> download_url=response.urljoin(url)</span><br><span class="line"> <span class="comment">#将url填入下载列表</span></span><br><span class="line"> item[<span class="string">'file_urls'</span>].append(download_url)</span><br><span class="line"> <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<p>当FilesPipeline下载完item[‘file_urls’]中的所有文件后，会将各文件的下载结果信息收集到另一个列表，赋给item的files字段(item[‘files’])，下载结果包括以下内容</p>
<p>Path 文件下载到本地的路径</p>
<p>Checksum 文件的校验和</p>
<p>url 文件的url地址</p>
<p>ImagesPipeline使用</p>
<p>导入路径  scrapy.pipelines.images.ImagesPipeline</p>
<p>Item字段  image_urls,images</p>
<p>下载目录  IMAGES_STORE</p>
<p>ImagesPipeline是在FilesPipeline基础上针对图片图片增加了一些特有功能</p>
<p>为图片生成缩略图</p>
<p>开启该功能，需要在setting中设置IMAGES_THUMBS  是一个字典，每一项的值是缩略图的尺寸</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_THUMBS=&#123; <span class="string">'small'</span>:(<span class="number">50</span>,<span class="number">50</span>), <span class="string">'big'</span>:(<span class="number">270</span>,<span class="number">270</span>),&#125;</span><br></pre></td></tr></table></figure>

<p>开启该功能后，下载一张图片，本地会出现三张图片，分别在full、thumbs/small下 thumbs/big下</p>
<p>过滤掉尺寸过小的图片</p>
<p>在setting.py中设置IMAGES_MIN_WIDTH和IMAGES_MIN_HEIGHT</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_MIN_WIDTH=<span class="number">110</span>IMAGES_MIN_HEIGHT=<span class="number">110</span></span><br></pre></td></tr></table></figure>

<p>开启该功能后，如果下载了一张105<img src="https://s.w.org/images/core/emoji/11/svg/2716.svg" srcset="/img/loading.gif" alt="✖">200的图片，该图片就会被抛弃掉，因为宽度不符合</p>
<p>文件下载例子</p>
<p>爬取matplotlib网站源码文件  html解析下载文件</p>
<p>创建项目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject matplotlib_examples</span><br></pre></td></tr></table></figure>

<p>到项目目录下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd matplotlib_examples/</span><br></pre></td></tr></table></figure>

<p>爬虫模板</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider examples matplotlib.org</span><br></pre></td></tr></table></figure>

<p><img src="http://47.93.248.15/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-02-%E4%B8%8B%E5%8D%886.07.23.png" srcset="/img/loading.gif" alt="img"></p>
<p>编辑items.py增加模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExampleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">     file_urls=scrapy.Field()</span><br><span class="line">     files=scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>编辑examples.py实现爬虫逻辑</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"> <span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"> <span class="keyword">from</span> matplotlib_examples.items <span class="keyword">import</span> ExampleItem</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">ExamplesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">     name = <span class="string">'examples'</span></span><br><span class="line">     allowed_domains = [<span class="string">'matplotlib.org'</span>]</span><br><span class="line">     start_urls = [<span class="string">'http://matplotlib.org/examples/index.html'</span>]</span><br><span class="line"> </span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">         le =LinkExtractor(restrict_css=<span class="string">'div.toctree-wrapper.compound'</span>,deny=<span class="string">'/index.html$'</span>)</span><br><span class="line">         <span class="keyword">for</span> link <span class="keyword">in</span> le.extract_links(response):</span><br><span class="line">             <span class="keyword">yield</span> scrapy.Request(link.url,callback=self.parse_example)</span><br><span class="line"> </span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(self,response)</span>:</span></span><br><span class="line">         href = response.css(<span class="string">'a.reference.external::attr(href)'</span>).extract_first()</span><br><span class="line">         url =response.urljoin(href)</span><br><span class="line">         example=ExampleItem()</span><br><span class="line">         print(url)</span><br><span class="line">         example[<span class="string">'file_urls'</span>]=[url]</span><br><span class="line">         <span class="keyword">return</span> example</span><br></pre></td></tr></table></figure>

<p>在setting.py中增加下载模块，下载模块会自动寻找file_urls字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">     <span class="string">'scrapy.pipelines.files.FilesPipeline'</span>:<span class="number">1</span></span><br><span class="line"> &#125;</span><br><span class="line"> FILES_STORE=<span class="string">'examples_src'</span></span><br></pre></td></tr></table></figure>

<p>增加run.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf8</span></span><br><span class="line"> <span class="keyword">from</span>  scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"> execute(<span class="string">'scrapy crawl examples -o examples.json'</span>.split())</span><br></pre></td></tr></table></figure>

<p>运行run.py</p>
<p>最后产生一个examples.json文件和一个examples_src文件夹，examples.json文件保存抓取下的数据,文件夹中保存的下载后的文件</p>
<p><img src="http://47.93.248.15/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-02-%E4%B8%8B%E5%8D%886.08.16.png" srcset="/img/loading.gif" alt="img"></p>
<p>但是下载的内容，会全部保存在full这个我们指定的文件夹下，但是文件名称为一段很长的sha散列值的文件，这是为了防止文件覆盖，但是这样很不方便阅读</p>
<p>我们期望每个文件都存入我们指定的地方，实际上FilesPipline里面的file_path方法决定了文件名，所以就需要我们继承FilesPipline重写file_path的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> <span class="keyword">from</span> os.path <span class="keyword">import</span>  basename,dirname,join</span><br><span class="line"> <span class="keyword">from</span> scrapy.pipelines.files <span class="keyword">import</span> FilesPipeline</span><br><span class="line"> </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">MyFilePipeline</span><span class="params">(FilesPipeline)</span>:</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">         path=urlparse(request.url).path</span><br><span class="line">         <span class="keyword">return</span> join(basename(dirname(path)),basename(path))</span><br></pre></td></tr></table></figure>

<p>在setting.py中添加的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;     <span class="comment"># 'scrapy.pipelines.files.FilesPipeline':1     'toscrape_book.pipelines.MyFilesPipeline': 300,  &#125;</span></span><br></pre></td></tr></table></figure>

<p>重新运行后，则展示了对应的文件在对应的文件夹下</p>
<p>此例子可以应用在下载对应的组图中</p>
<p>360图片下载项目   json+下载项目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject so_image</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd so_image</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider images image.so.com</span><br></pre></td></tr></table></figure>

<p>使用pycharm打开</p>
<p>在setting.py中添加ImagesPipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;     </span><br><span class="line"><span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>:<span class="number">1</span> </span><br><span class="line">&#125; IMAGES_STORE=<span class="string">'download_images'</span></span><br></pre></td></tr></table></figure>

<p>实现imagesSpider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"> <span class="keyword">import</span> json</span><br><span class="line"> <span class="keyword">from</span> scrapy <span class="keyword">import</span>  Request</span><br><span class="line"> </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">ImagesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">     BASE_URL=<span class="string">'http://image.so.com/zj?ch=art&amp;sn=%s&amp;listtype=new&amp;temp=1'</span></span><br><span class="line">     <span class="comment">#限制最大下载量，防止磁盘用量过大</span></span><br><span class="line">     MAX_DOWNLOAD_NUM=<span class="number">1000</span></span><br><span class="line">     name = <span class="string">'images'</span></span><br><span class="line">     allowed_domains = [<span class="string">'image.so.com'</span>]</span><br><span class="line">     start_urls = [BASE_URL%<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">         <span class="comment">#使用json模块解析响应结果</span></span><br><span class="line">         infos= json.loads(response.body.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">         <span class="comment">#提取所有的图片下载url到一个列表，赋值给item的image_urls字段</span></span><br><span class="line">         <span class="keyword">yield</span> &#123;<span class="string">'image_urls'</span>:[info[<span class="string">'qhimg_url'</span>] <span class="keyword">for</span> info <span class="keyword">in</span> infos[<span class="string">'list'</span>]]&#125;</span><br><span class="line">         <span class="comment">#如count字段大于0，并且下载数据不足MAX_DOWNLOAD_NUM,继续获取下一页图片信息</span></span><br><span class="line">         self.start_index+=infos[<span class="string">'count'</span>]</span><br><span class="line">         <span class="keyword">if</span> infos[<span class="string">'count'</span>] &gt;<span class="number">0</span> <span class="keyword">and</span> self.start_index&lt;self.MAX_DOWNLOAD_NUM:</span><br><span class="line">             <span class="keyword">yield</span> Request(self.BASE_URL%self.start_index)</span><br></pre></td></tr></table></figure>

<p>在setting.py中设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>创建运行run.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>  scrapy.cmdline <span class="keyword">import</span> execute </span><br><span class="line">execute([<span class="string">'scrapy'</span>,<span class="string">'crawl'</span>,<span class="string">'images'</span>])</span><br></pre></td></tr></table></figure>

<p>查看结果，可以看到下载的图片</p>

            <hr>
          </div>
          <br>
          <div>
            
              <p>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E7%88%AC%E8%99%AB">爬虫</a>
                  &nbsp;
                
                  <a class="hover-with-bg" href="/categories/scrapy-5-%E6%96%87%E4%BB%B6%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD">scrapy-5-文件图片下载</a>
                  &nbsp;
                
              </p>
            
            <p>
              <i class="iconfont icon-tag"></i>
              
                <a class="hover-with-bg" href="/tags/python">python</a>
              
                <a class="hover-with-bg" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
              
                <a class="hover-with-bg" href="/tags/scrapy">scrapy</a>
              
                <a class="hover-with-bg" href="/tags/scrapy-5-%E6%96%87%E4%BB%B6%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD">scrapy-5-文件图片下载</a>
              
              <span id="/2016/12/03/scrapy-5-文件图片下载/" class="visitors leancloud_visitors" data-flag-title="scrapy-5-文件图片下载">
                <em id="visitors-text" class="post-meta-item-text"></em>
                <i id="visitors-count"></i>
              </span>
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>
    
  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>


  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>


  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "scrapy-5-文件图片下载&nbsp;",
      ],
      cursorChar: "The greatest generosity to the future is to give something",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "true",
      
      icon: "❡"
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $('#post').find('img').each(
      function () {
        var _this = $(this);
        var _src = _this.attr("src");
        _this.wrap('<a data-fancybox="images" href="' + _src + '" ></a>');
      }
    );
  </script>




</body>
</html>
