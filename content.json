{"pages":[],"posts":[{"title":"Excel-openpyxl-3-增删改查","text":"Excel-openpyxl-3-增删改查通过这篇文章，你能学习到以下内容1、openpyxl的增加数据2、openpyxl删除数据3、更新数据4、查找数据 1、openpyxl的增加数据 使用上一个文件创建的create_excel.xlsx create_excel 原始文件，数据源 openpyxledit.py 相关代码 每次调用，在最后一列，多增加一列数据 引用模块openpyxl 1from openpyxl import load_workbookfrom openpyxl import Workbook# 一个eggache的数字转为列字母的方法from openpyxl.utils import get_column_letter 目标:调用一次，最后一列增加一列 思路: 获得最后一列或者一行的位置 增加一列:通过cell指定最后一列的位置增加1，依次对这一列的每一行赋值 增加一行:通过cell指定最后一行位置增加1，依次对这行的每一列进行赋值 1#增加数据def addExcel(): # 打开一个工作薄 wb = load_workbook(&apos;create_excel.xlsx&apos;, data_only=True) # data_only=True是关键，显示公式结果 # 获取一张表，需要对应那个表的name，之前取的是create_sheet sheet = wb.get_sheet_by_name(&quot;create_sheet&quot;) #获得最大行数 row=sheet.max_row #获得最大列数 column=sheet.max_column #循环插入一列数据 for i in range(row): #列数+1转换为字母 h_col = get_column_letter(column+1) #行数从1开始,从0会报错 xx=h_col+str(i+1) #可以使用sheet[xx].value=xx方式进行赋值 # sheet[xx].value=xx #row是数字，column是数字，都是从1开始 sheet.cell(row=(i+1),column=column+1).value=xx wb.save(&quot;create_excel.xlsx&quot;) 调用 1addExcel() 2、 openpyxl删除数据 目标:调用一次，最后一行删除一行 思路： openpyxl本身不提供删除，原因在于删除涉及移动数据，但是如果行或者列的数据本身都为None时候，默认这行就是没有的，利用这个规则，可以把一行或者一列的数据全部设置为None即可删除 1def deleteExcel(): # 打开一个工作薄 wb = load_workbook(&apos;create_excel.xlsx&apos;, data_only=True) # data_only=True是关键，显示公式结果 # 获取一张表，需要对应那个表的name，之前取的是create_sheet sheet = wb.get_sheet_by_name(&quot;create_sheet&quot;) # 获得最大行数 row = sheet.max_row # 获得最大列数 column = sheet.max_column print(column) # 删除最后一列数据 #for i in range(row): # sheet.cell(row=(i+1), column=column).value = None # 删除最后一行数据 for i in range(column): sheet.cell(row=row,column=i+1).value=None wb.save(&quot;create_excel.xlsx&quot;) 调用删除数据 1deleteExcel() 3、 更新数据 目标:修改任意数据，进行保存 思路 基本思路和增加是一样的，但是比较简单的是不获取一列数据，而是单独的一个cell 对单独cell赋值后进行保存，即可完成更新 1def updateExcel(): # 打开一个工作薄 wb = load_workbook(&apos;create_excel.xlsx&apos;, data_only=True) # data_only=True是关键，显示公式结果 # 获取一张表，需要对应那个表的name，之前取的是create_sheet sheet = wb.get_sheet_by_name(&quot;create_sheet&quot;) sheet[&apos;C3&apos;] = &apos;new data&apos; wb.save(&quot;create_excel.xlsx&quot;) 调用更新数据 1updateExcel() 4、 查找数据 目标:查找所有数据 思路 获得最大列和最大行，进行遍历，需要注意行和列都是从1开始的 1def selectExcel(): # 打开一个工作薄 wb = load_workbook(&apos;create_excel.xlsx&apos;,data_only=True) #data_only=True是关键，显示公式结果 # 获取一张表，需要对应那个表的name，之前取的是create_sheet sheet=wb.get_sheet_by_name(&quot;create_sheet&quot;) # 获取A1格的值 print(&quot;A1的值 &quot;,sheet[&apos;A1&apos;].value) # 获取表的内容的最大工作行数 print(&quot;表最大工作行数 &quot;,sheet.max_row) # 获取表的内容的最大工作列数 print(&quot;表最大工作列数 &quot;,sheet.max_column) # 遍历表中所有内容 for row in sheet.rows: #row是每一行中的数据 for cell in row: print(cell.value,end=&quot; &quot;) print(&quot;&quot;) 调用查询 1selectExcel()","link":"/2017/09/30/Excel-openpyxl-3-增删改查/"},{"title":"Excel-openpyxl-创建和读取","text":"####Excel-openpyxl-创建和读取 通过这篇文章你能够学习到以下内容** 操作excel有哪些库 openpyxl是什么 openpyxl读取、创建excel 1、操作**excel**有哪些库 wlrd：只能处理65535行内的数据，如果数据量超过65535行，则会出现报错 2、openpyxl是什么 openpyxl模块是一个对Excel操作Python库，能够处理更多数据的内容，但是相对与xlrd，openpyxl的读取效率要远远低于xlrd 以上需要根据实际业务需求进行选择，这里主要讲解openpyxl库 首先先清楚一些excel的基本概念： 在openpyxl中，主要用到三个概念：Workbooks，Sheets，Cells。Workbook就是一个excel工作表；Sheet是工作表中的一张表页；Cell就是简单的一个格。openpyxl就是围绕着这三个概念进行的，不管读写都是“三板斧”：打开Workbook，定位Sheet，操作Cell。 总结: workbooks是excel一个工作表，一个工作表包含多个页，即包含多个sheet sheet是excel当中的一页，一个工作页包含多个表格，即包含多个cell cell是excel当中的一个工作表格 3、openpyxl读取、创建excel openpyxl读写、创建代码下载 3.1 安装 1pip install openpyxl (简便安装:使用pycharm，导入模块，按alt+回车自动安装，其实后面实现也是pip install openpyxl) 3.2 创建Excel 1from openpyxl import load_workbookfrom openpyxl import Workbookdef createExcel(): #创建一个工作薄 wb = Workbook() #通过工作簿创建一个工作表，默认是第一个工作表 sheet = wb.active #可以通过create_sheet来指定创建第几个sheet #ws = wb.create_sheet(1,&apos;custom&apos;) # excel创建的工作表名默认为sheet1 sheet.title = &apos;create_sheet&apos; # 向工作表中输入内容(C3对应是ExcelC3的位置) sheet[&apos;C3&apos;] = &apos;hello word&apos; #对A列1-10输入内容 for i in range(10): sheet[&apos;A%d&apos; % (i+1)] = i+1 # 向excel表中输入表达式 C2显示内容为A列求和 sheet[&apos;C2&apos;] = &apos;=SUM(A:A)&apos; # 保存一个文档 wb.save(&apos;create_excel.xlsx&apos;)createExcel() 这样在这个py文件的同一文件夹下，会生成一个create_excel.xlsx 3.3 读取Excel 在刚才我们创建了这个Excel文件，现在来进行读取Excel文件 1def loadExcel(): # 打开一个工作薄 wb = load_workbook(&apos;create_excel.xlsx&apos;,data_only=True) #data_only=True是关键，显示公式结果 # 获取一张表，需要对应那个表的name，之前取的是create_sheet sheet=wb.get_sheet_by_name(&quot;create_sheet&quot;) # 获取A1格的值 print(&quot;A1的值 &quot;,sheet[&apos;A1&apos;].value) # 获取表的内容的最大工作行数 print(&quot;表最大工作行数 &quot;,sheet.max_row) # 获取表的内容的最大工作列数 print(&quot;表最大工作列数 &quot;,sheet.max_column) # 遍历表中所有内容 for row in sheet.rows: #row是每一行中的数据 for cell in row: print(cell.value,end=&quot; &quot;) print(&quot;&quot;) # row[2].valueloadExcel() 显示结果如下 1A1的值 1表最大工作行数 10表最大工作列数 31 None None 2 None 55 3 None hello word 4 None None 5 None None 6 None None 7 None None 8 None None 9 None None 10 None None 通过结果，会发现最大行数和最大列数指的是我们文档中写入的数据最后一行和最后一列","link":"/2017/08/05/Excel-openpyxl-创建和读取/"},{"title":"Excel-openpyxl--数据梳理","text":"通过这篇文章，你能学习到以下内容 1、读取文件2、整理数据结构:字典类型 key是班级id value 是数组类型，数组里面对应的是字典，对应每一行数据3、遍历所有value，读取的数组传入到一个函数中4、函数中处理计数变量，开始遍历数组5、在函数中操作全局变量，记录整理好的数据6、把整理好的数据写入 1、需求： 读取基础数据然后按照结果模板进行处理，数据中是按照班级进行分组统计 基础数据一次运算消耗时间太大，简化数据，使用基础数据1来调试程序 2、思路： 2.1、读取文件2.2、整理数据结构:字典类型 key是班级id value 是数组类型，数组里面对应的是字典，对应每一行数据2.3、遍历所有value，读取的数组传入到一个函数中2.4、函数中处理计数变量，开始遍历数组2.5、在函数中操作全局变量，记录整理好的数据2.6、把整理好的数据写入 3、实现 完整代码 读取Excel数据，梳理数据，把id相同的保存在一起，以每行数据作为一个对象，统一操作 1234567import sysimport osfrom openpyxl import load_workbookfrom openpyxl import Workbook# 一个eggache的数字转为列字母的方法from openpyxl.utils import get_column_letterfrom openpyxl.reader.excel import load_workbook 123456789101112131415161718192021#读取Excel数据 dicdata={} def readExcel(file_name): wb2=load_workbook(file_name,data_only=True) #读取excel data_only=True是关键，显示公式结果 ws2=wb2.get_sheet_by_name(\"收入确认\")#wb2.active是默认第一个 # ws2=wb2.active#wb2.active是默认第一个 #读取每一行内容 i=0 #注 ：ws2.rows[i] 这种方式特别耗时 for row in ws2.rows: i=i+1 if i&lt;4: continue classid=str(row[2].value) arr = dicdata.get(classid) if arr == None: dicdata[classid] = [row]#如果空就新增一个id对应 else: arr.append(row)# 追加数据即可 数据处理，对每一个班级内数据进行合并计算 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#读取Excel数据 dicdata={} def readExcel(file_name): wb2=load_workbook(file_name,data_only=True) #读取excel data_only=True是关键，显示公式结果 ws2=wb2.get_sheet_by_name(\"收入确认\")#wb2.active是默认第一个 # ws2=wb2.active#wb2.active是默认第一个 #读取每一行内容 i=0 #注 ：ws2.rows[i] 这种方式特别耗时 for row in ws2.rows: i=i+1 if i&lt;4: continue classid=str(row[2].value) arr = dicdata.get(classid) if arr == None: dicdata[classid] = [row]#如果空就新增一个id对应 else: arr.append(row)# 追加数据即可 数据处理，对每一个班级内数据进行合并计算def handle_data(arr,classid): 校区=arr[0][1].value 班级id=classid #班级名字没有，用班级类型替代 班级名字=arr[0][9].value 班级类型=arr[0][9].value 学科=arr[0][10].value 授课模式=arr[0][17].value 开班日期=arr[0][13].value 预计毕业时间=arr[0][14].value 毕业时间=arr[0][15].value 预计毕业总时间=arr[0][18].value 执行价格合计 = 优惠金额小计 = 应交学费合计 =0 新生 = 升级 = 转班来 = 退学归来 = 重读 = 留级 = 退学 = 转班走 = 休学 = 开除 = 0 收款合计 = 收款合计2015 = 收款合计2016 = 收款合计2017 = 收款合计2018 = 0 班级已确认收入小计 = 收入确认培训2015 = 收入确认培训2016 = 收入确认培训2017 = 收入确认培训2018 = 0 现金折扣合计 = 现金折扣2015 = 现金折扣2016 = 现金折扣2017 = 现金折扣2018 = 0 #记录后面44-298的所有内容 arrtest=[0 for i in range(38,len(arr[0]))] #遍历所有row for row in arr: 执行价格合计=执行价格合计+row[3].value 优惠金额小计=优惠金额小计+row[7].value 应交学费合计=应交学费合计+row[8].value #判断状态 status=row[11].value if status == \"新生\": 新生=新生+1 if status == \"升级\": 升级=升级+1 if status == \"转班来\": 转班来=转班来+1 if status == \"退学归来\": 退学归来=退学归来+1 if status == \"重读\": 重读=重读+1 if status == \"留级\": 留级=留级+1 if status == \"退学\": 退学=退学+1 if status == \"转班走\": 转班走=转班走+1 if status == \"休学\": 休学=休学+1 if status == \"开除\": 开除=开除+1 收款合计=收款合计+row[20].value 收款合计2015=收款合计2015+row[22].value 收款合计2016=收款合计2016+row[23].value 收款合计2017=收款合计2017+row[24].value 收款合计2018 = 收款合计2018 + row[25].value 班级已确认收入小计=班级已确认收入小计+row[26].value 收入确认培训2015=收入确认培训2015+row[28].value 收入确认培训2016=收入确认培训2016+row[29].value 收入确认培训2017=收入确认培训2017+row[30].value 收入确认培训2018=收入确认培训2018+row[31].value 现金折扣合计=现金折扣合计+row[32].value 现金折扣2015=现金折扣2015+row[34].value 现金折扣2016 = 现金折扣2016 + row[35].value 现金折扣2017 = 现金折扣2017 + row[36].value 现金折扣2018 = 现金折扣2018 + row[37].value # 执行价格合计 = 优惠金额小计 = 应交学费合计 0 # 新生 = 升级 = 转班来 = 退学归来 = 重读 = 留级 = 退学 = 转班走 = 休学 = 开除 = 0 # 收款合计 = 收款合计2015 = 收款合计2016 = 收款合计2017 = 收款合计2018 = 0 # 班级已确认收入小计 = 收入确认培训2015 = 收入确认培训2016 = 收入确认培训2017 = 收入确认培训2018 = 0 # 现金折扣合计 = 现金折扣2015 = 现金折扣2016 = 现金折扣2017 = 现金折扣2018 = 0 #开始循环剩下的字段 for i in range(38,len(row)): if row[i].value == None: continue value=arrtest[i-38];#读取 value=value+row[i].value#累加 arrtest[i-38]=value # 保存前面所有累加数据 result_arr = [校区, 班级id, 班级名字, 班级类型, 学科, 授课模式, 开班日期, 预计毕业时间, 毕业时间, 预计毕业总时间, 执行价格合计, 优惠金额小计, 应交学费合计, 新生, 升级, 转班来, 退学归来, 重读, 留级, 退学, 转班走, 休学, 开除, 收款合计, 收款合计2015, 收款合计2016, 收款合计2017, 收款合计2018, 班级已确认收入小计, 收入确认培训2015, 收入确认培训2016, 收入确认培训2017, 收入确认培训2018, 现金折扣合计, 现金折扣2015, 现金折扣2016, 现金折扣2017, 现金折扣2018] #获得的最终每条的总数据 result_arr=result_arr+arrtest return result_arr 写入数据 123456789101112131415161718192021222324def write_to_excel_with_openpyxl(result_arr,head_row=None,save_excel_name=\"save.xlsx\"): page = 0 # 新建一个workbook wb = Workbook() # 设置文件输出路径与名称 dest_filename = save_excel_name # 第一个sheet是ws ws = wb.create_sheet(0,page) # # 写第一行，标题行 # for h_x in range(1, len(head_row) + 1): # h_col = get_column_letter(h_x) # # print h_col # ws.cell('%s%s' % (h_col, 1)).value = '%s' % (head_row[h_x - 1]) page += 1 if len(result_arr) == 0: return for i in range(len(result_arr)): # 对result的每个子元素作遍历， for j in range(len(result_arr[i])): #列是从1开始计算的 ws.cell(row=i+1,column=j+1).value = result_arr[i][j] print(\"正在写入，进行保存\",dest_filename) wb.save('save.xlsx') # wb.save(filename=(os.path.dirname(sys.argv[0])+'/save.xlsx')) 最后进行遍历调用每一个数据，保存在数组中进行写入 1234567891011121314import os# print(sys.argv[0])#当前文件路径path =os.path.dirname(sys.argv[0])print (path)#上一级目录 readExcel('基础数据1.xlsx')#读取了数据allresult_arr=[] for key in dicdata: result_arr= handle_data(dicdata[key],key) allresult_arr.append(result_arr)print(\"准备开始写入数据\")write_to_excel_with_openpyxl(allresult_arr)print(\"写入完成\")","link":"/2017/09/26/Excel-openpyxl-数据梳理/"},{"title":"Python-打包exe","text":"Python-5-打包exe通过这篇文章，你能学习到以下内容 1、使用pyinstaller进行打包 打包exe文件,使用pyinstaller，pyinstaller支持python2和python3 目前存在问题: 1、pyinstaller 打包是在什么环境下打包，可以在什么环境下执行，无法在Mac下打包exe 2、执行后大量占用内存，最后无法执行完成(有待修复) 安装 12pip install pyinstallerpip install --upgrade pyinstaller 将cmd的目录切换至（命令：cd 文件路径(注意空格)）需要打包的py文件目录下： 1pyinstaller -F MyTools.py 参数说明 123456–icon=图标路径-F 打包成一个exe文件-w 使用窗口，无控制台-c 使用控制台，无窗口-D 创建一个目录，里面包含exe以及其他一些依赖性文件pyinstaller -h 来查看参数 回车后，直到操作结束。返回目标文件目录，发现该目录下生成了.spec文件test.spec：打包好的exe文件，在同目录的dist文件中：my.ico 是一个图标名，和当前的test.py文件在同一个目录下 需要注意的是如果读取文件，需要修改相应代码 12345678910111213141516import osprint(sys.argv[0])#当前文件路径path =os.path.dirname(sys.argv[0])print (path)#上一级目录for filename in os.listdir(path): if filename.find(\".xlsx\") &gt;= 0: print(\"正在读取数据文件~%s\"%filename) readExcel(path+'/'+filename)#读取了数据 print(\"读取数据完成。。。开始梳理数据中\") #循环所有key allresult_arr=[] for key in dicdata: result_arr= handle_data(dicdata[key],key) allresult_arr.append(result_arr) print(\"准备开始写入数据\") writetoexcel(allresult_arr)","link":"/2016/05/02/Python-打包exe/"},{"title":"docker","text":"####Python操作docker 通过python调用docker，需要和docker在同一个服务器上才可以 也可以通过传递 1unix:///var/run1/docker.sock 来进行在容器内操作你指定想要连接的docker，这个共享需要在容器最开始创建时候，让容器和外部一个文件进行关联才可以，具体可以参照一下 docker run -d -it -p 805:80 -p 5002:5000 -p 10027:22 -p 20001:8888 -p 20002:3306 –name centos_master –privileged=true -v /var/run/docker.sock:/var/run/docker.sock -v /zc:/zc -e LANG=zh_CN.utf8 zhangcheng0111/centos7-webssh-dockermanager-ssh /usr/sbin/init 其中比较关键的是-v /var/run/docker.sock:/var/run/docker.sock 映射了容器内部和外部的关联文件 以下介绍通过python控制 官方文档地址https://github.com/docker/docker-py 只能作为参考，写的很简单 docker-py实际上分为很多版本，新版和旧版的api不一样，新版的删除掉了很多实用的方法，并且没有经过实际生产环境验证，无法得知具体使用效果，这里所说的是基于旧版的1.19版本 安装docker-py 1pip install docker-py 一、初始化docker客户端 1import docker #初始化有2种方式都可以 #第一种 1client = docker.Client(base_url=&apos;unix:///var/run/docker.sock&apos;,version=&apos;1.19&apos;) 其中version指定使用的版本，很重要！！！一定不能少 #第二种 1client=docker.from_env(version=&apos;1.19&apos;) #推荐使用第一种方式 二、获得镜像信息 1print(clinet.images()) 返回的数据是列表形式 拉去镜像 1print(clinet.pull(&apos;zhangcheng0111/centos7-webssh-dockermanager-ssh&apos;)) 三、根据镜像创建一个容器 1client.create_container(image=&apos;zhangcheng0111/centos7-webssh-dockermanager-ssh&apos;, name=&apos;test3&apos;, ports=[50070,8088,9000,3306,9999,16010,22,5000,8080,80,8888], command=&quot;/usr/sbin/init&quot;, environment={&quot;LANG&quot;: &quot;zh_CN.utf8&quot;}, hostname=&apos;zc01&apos;, host_config=self.client.create_host_config(privileged=True,port_bindings=dic1)) 参数说明 image=’zhangcheng0111/centos7-webssh-dockermanager-ssh’ 是你pull的镜像,这个镜像的地址会从hub.docker.com上查找对应的镜像 name=’test3′ 很重要，以后启动容器就靠它了 command=’/usr/sbin/init’ 启动镜像后的命令行，实际上就是进行一些基础服务的启动 volumes=[‘/data’] 数据卷，相当于数据在哪保存，路径是宿主机的，这里没有使用 environment={“LANG”: “zh_CN.utf8”} 设定一些参数 ports=[80,22] 额外开启哪些端口 privileged=True 设置特权启动，让你容器具备这个容器的root权限 port_bindings=dic1 设定具体内部和外部端口的映射{’80’:None}如果采用None，则docker会自动进行分配端口，如果自己手动指定，需要注意不要端口冲突，否则会创建失败的 如果自动分配了，我们如何获得这个容器的外部映射端口号呢？后面会说如何查看这个容器配置，包括内网ip和外网ip 这样我们就创建了一个容器，创建了容器还没有开启 四、获得所有镜像 1print(client.containers(all=True)) 结果如下 1[{u&apos;Status&apos;: u&apos;Exited (0) About a minute ago&apos;, u&apos;Created&apos;: 1532098433, u&apos;Image&apos;: u&apos;sequenceiq/spark:1.6.0&apos;, u&apos;Labels&apos;: {}, u&apos;NetworkSettings&apos;: {u&apos;Networks&apos;: {u&apos;bridge&apos;: {u&apos;NetworkID&apos;: u&apos;66af7ecb2d04d21dba5bb003a21c2adad034430315cc380fff718c9d7f6e29c3&apos;, u&apos;MacAddress&apos;: u&apos;&apos;, u&apos;GlobalIPv6PrefixLen&apos;: 0, u&apos;Links&apos;: None, u&apos;GlobalIPv6Address&apos;: u&apos;&apos;, u&apos;IPv6Gateway&apos;: u&apos;&apos;, u&apos;DriverOpts&apos;: None, u&apos;IPAMConfig&apos;: None, u&apos;EndpointID&apos;: u&apos;&apos;, u&apos;IPPrefixLen&apos;: 0, u&apos;IPAddress&apos;: u&apos;&apos;, u&apos;Gateway&apos;: u&apos;&apos;, u&apos;Aliases&apos;: None}}}, u&apos;HostConfig&apos;: {u&apos;NetworkMode&apos;: u&apos;default&apos;}, u&apos;ImageID&apos;: u&apos;sha256:40a687b3cdccd2224016c07a5ca20aeca0d0ef9f397d11ae6102ee1245d915d6&apos;, u&apos;State&apos;: u&apos;exited&apos;, u&apos;Command&apos;: u&apos;/etc/bootstrap.sh&apos;, u&apos;Names&apos;: [u&apos;/test3&apos;], u&apos;Mounts&apos;: [], u&apos;Id&apos;: u&apos;d322d097f34c0c18cec0a3dcc863ace22ac14c07e4ff3aaeb4f52816e9a87f69&apos;, u&apos;Ports&apos;: []}, {u&apos;Status&apos;: u&apos;Exited (0) 2 hours ago&apos;, u&apos;Created&apos;: 1532097805, u&apos;Image&apos;: u&apos;sequenceiq/spark:1.6.0&apos;, u&apos;Labels&apos;: {}, u&apos;NetworkSettings&apos;: {u&apos;Networks&apos;: {u&apos;bridge&apos;: {u&apos;NetworkID&apos;: u&apos;66af7ecb2d04d21dba5bb003a21c2adad034430315cc380fff718c9d7f6e29c3&apos;, u&apos;MacAddress&apos;: u&apos;&apos;, u&apos;GlobalIPv6PrefixLen&apos;: 0, u&apos;Links&apos;: None, u&apos;GlobalIPv6Address&apos;: u&apos;&apos;, u&apos;IPv6Gateway&apos;: u&apos;&apos;, u&apos;DriverOpts&apos;: None, u&apos;IPAMConfig&apos;: None, u&apos;EndpointID&apos;: u&apos;&apos;, u&apos;IPPrefixLen&apos;: 0, u&apos;IPAddress&apos;: u&apos;&apos;, u&apos;Gateway&apos;: u&apos;&apos;, u&apos;Aliases&apos;: None}}}, u&apos;HostConfig&apos;: {u&apos;NetworkMode&apos;: u&apos;default&apos;}, u&apos;ImageID&apos;: u&apos;sha256:40a687b3cdccd2224016c07a5ca20aeca0d0ef9f397d11ae6102ee1245d915d6&apos;, u&apos;State&apos;: u&apos;exited&apos;, u&apos;Command&apos;: u&apos;/etc/bootstrap.sh&apos;, u&apos;Names&apos;: [u&apos;/test2&apos;], u&apos;Mounts&apos;: [], u&apos;Id&apos;: u&apos;089dbc622b0409ed70fb6c2bdf735e0edc70282aa2c7f4d0d6b4092d74aa05be&apos;, u&apos;Ports&apos;: []}, {u&apos;Status&apos;: u&apos;Exited (137) 2 hours ago&apos;, u&apos;Created&apos;: 1532097064, u&apos;Image&apos;: u&apos;sequenceiq/spark:1.6.0&apos;, u&apos;Labels&apos;: {}, u&apos;NetworkSettings&apos;: {u&apos;Networks&apos;: {u&apos;bridge&apos;: {u&apos;NetworkID&apos;: u&apos;66af7ecb2d04d21dba5bb003a21c2adad034430315cc380fff718c9d7f6e29c3&apos;, u&apos;MacAddress&apos;: u&apos;&apos;, u&apos;GlobalIPv6PrefixLen&apos;: 0, u&apos;Links&apos;: None, u&apos;GlobalIPv6Address&apos;: u&apos;&apos;, u&apos;IPv6Gateway&apos;: u&apos;&apos;, u&apos;DriverOpts&apos;: None, u&apos;IPAMConfig&apos;: None, u&apos;EndpointID&apos;: u&apos;&apos;, u&apos;IPPrefixLen&apos;: 0, u&apos;IPAddress&apos;: u&apos;&apos;, u&apos;Gateway&apos;: u&apos;&apos;, u&apos;Aliases&apos;: None}}}, u&apos;HostConfig&apos;: {u&apos;NetworkMode&apos;: u&apos;default&apos;}, u&apos;ImageID&apos;: u&apos;sha256:40a687b3cdccd2224016c07a5ca20aeca0d0ef9f397d11ae6102ee1245d915d6&apos;, u&apos;State&apos;: u&apos;exited&apos;, u&apos;Command&apos;: u&apos;/etc/bootstrap.sh&apos;, u&apos;Names&apos;: [u&apos;/test1&apos;], u&apos;Mounts&apos;: [], u&apos;Id&apos;: u&apos;a6b4592de281345d4f5c7e3c7254048214fdeb9581bba13d37aa612cfeff7a8d&apos;, u&apos;Ports&apos;: []}] 五、开启容器 1client.statr(container=&apos;test3&apos;,port_bindings={80:80,22:2022}) container=’test3′ 创建容器设置的name，用于开启容器port_bindings={80:80,22:2022} 设置容器与宿主机端口之间的映射关系，这里port_bindings我测试过只能修改在你创建时候设定的端口，如果没有开放的端口，则无法修改，所以在这里一般没什么卵用publish_all_ports=True 全端口公开，会依次进行映射外部访问以下参数，没搞明白是干啥的lxc_conf=None 不知道啥意思links=Noneprivileged=Falsedns=Nonedns_search=Nonevolumes_from=Nonenetwork_mode=Nonerestart_policy=Nonecap_add=Nonecap_drop=None 六、停止容器 1client.stop(container=&apos;test3&apos;) 七、查看容器的具体参数 1item = self.client.containers(filters={&apos;name&apos;: &apos;test3&apos;})[0]# 遍历获得对应的容器的ip和端口 22和8888的映射端口 ip = item[&apos;NetworkSettings&apos;][&apos;Networks&apos;][&apos;bridge&apos;][&apos;IPAddress&apos;]with open(&quot;/zc/ip.txt&quot;) as f: wip = f.read().strip(&quot;\\n&quot;)print(&apos;外网地址&apos;, wip)print(&apos;内网地址&apos;, ip)# 对应key-value是数字# 遍历获得对应的容器的ip和端口 22和8888的映射端口for port in item[&apos;Ports&apos;]: dic[port[&apos;PrivatePort&apos;]] = port.get(&apos;PublicPort&apos;) 通过以上我们不难看出通过返回的数据，可以遍历里面的ip地址 至于/zc/ip.txt是这个什么鬼？这个是我外部与内部共享的一个文件，用于获得外部的ip地址，因为容器本身只能获得自己的内网ip地址，无法得知外网地址","link":"/2017/04/25/docker/"},{"title":"Ubuntu MongoDB数据库的安装和使用","text":"本博文介绍了MongoDB，并详细指引读者在Ubuntu下MongoDB的安装和使用。本教程在Ubuntu14.04下测试通过。 一、MongoDB介绍MongoDB 是一个是一个基于分布式文件存储的数据库，介于关系数据库和非关系数据库之间，是非关系数据库当中功能最丰富，最像关系数据库的。他支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。Mongo最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。 二、安装MongoDBMongoDB安装很简单，无需下载源文件，可以直接用apt-get命令进行安装。打开终端，输入以下命令： 1sudo apt-get install mongodb 1.安装完成后，在终端输入以下命令查看MongoDB版本： 1mongo -version 输出版本信息，表明安装成功，截图如下： 2.启动和关闭mongodb命令如下： 12service mongodb startservice mongodb stop 截图如下： 3.默认设置MongoDB是随Ubuntu启动自动启动的。输入以下命令查看是否启动成功： 1pgrep mongo -l #注意：-l是英文字母l，不是阿拉伯数字1 截图如下：卸载MongoDB 1sudo apt-get --purge remove mongodb mongodb-clients mongodb-server 三、使用MongoDBshell命令模式输入mongo进入shell命令模式，默认连接的数据库是test数据库，在此之前一定要确保你已经启动了MongoDB，否则会出现错误，启动之后运行成功，如下截图：常用操作命令：数据库相关 123456show dbs:显示数据库列表 show collections：显示当前数据库中的集合（类似关系数据库中的表table） show users：显示所有用户 use yourDB：切换当前数据库至yourDB db.help() ：显示数据库操作命令 db.yourCollection.help() ：显示集合操作命令，yourCollection是集合名 MongoDB没有创建数据库的命令，如果你想创建一个“School”的数据库，先运行use School命令，之后做一些操作（如：创建聚集集合db.createCollection(‘teacher’)）,这样就可以创建一个名叫“School”的数据库。截图如下： 下面以一个School数据库为例，在School数据库中创建两个集合teacher和student，并对student集合中的数据进行增删改查基本操作（集合Collection相当于关系型数据库中的表table）。1、切换到School数据库 1use School #切换到School数据库。MongoDB 无需预创建School数据库，在使用时会自动创建 2、创建Collection 1db.createCollection(&apos;teacher&apos;) #创建一个聚集集合。MongoDB 其实在插入数据的时候，也会自动创建对应的集合，无需预定义集合 截图如下：3、插入数据与数据库创建类似，插入数据时也会自动创建集合。插入数据有两种方式：insert和save。 12db.student.insert({_id:1, sname: &apos;zhangsan&apos;, sage: 20}) #_id可选db.student.save({_id:1, sname: &apos;zhangsan&apos;, sage: 22}) #_id可选 这两种方式，其插入的数据中_id字段均可不写，会自动生成一个唯一的_id来标识本条数据。而insert和save不同之处在于：在手动插入_id字段时，如果_id已经存在，insert不做操作，save做更新操作；如果不加_id字段，两者作用相同都是插入数据。截图如下： 添加的数据其结构是松散的，只要是bson格式均可，列属性均不固定，根据添加的数据为准。先定义数据再插入，就可以一次性插入多条数据，截图如下：运行完以上例子，student 已自动创建，这也说明 MongoDB 不需要预先定义 collection ，在第一次插入数据后，collection 会自动的创建。截图如下： 4、查找数据 1db.youCollection.find(criteria, filterDisplay) criteria ：查询条件，可选filterDisplay：筛选显示部分数据，如显示指定列数据，可选（当选择时，第一个参数不可省略，若查询条件为空，可用{}做占位符，如下例第三句） 12345db.student.find() #查询所有记录。相当于：select * from studentdb.student.find({sname: &apos;lisi&apos;}) #查询sname=&apos;lisi&apos;的记录。相当于： select * from student where sname=&apos;lisi&apos;db.student.find({},{sname:1, sage:1}) #查询指定列sname、sage数据。相当于：select sname,sage from student。sname:1表示返回sname列，默认_id字段也是返回的，可以添加_id:0（意为不返回_id）写成{sname: 1, sage: 1,_id:0}，就不会返回默认的_id字段了db.student.find({sname: &apos;zhangsan&apos;, sage: 22}) #and 与条件查询。相当于：select * from student where sname = &apos;zhangsan&apos; and sage = 22db.student.find({$or: [{sage: 22}, {sage: 25}]}) #or 条件查询。相当于：select * from student where sage = 22 or sage = 25 查询操作类似，这里只给出db.student.find({sname: ‘lisi’})查询的截图，如下： 5、修改数据 123456db.youCollection.update(criteria, objNew, upsert, multi ) criteria: update的查询条件，类似sql update查询内where后面的 objNew : update的对象和一些更新的操作符（如$set）等，也可以理解为sql update查询内set后面的。 upsert : 如果不存在update的记录，是否插入objNew，true为插入，默认是false，不插入。 multi: mongodb默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。默认false，只修改匹配到的第一条数据。 其中criteria和objNew是必选参数，upsert和multi可选参数 举例如下： 1db.student.update({sname: &apos;lisi&apos;}, {$set: {sage: 30}}, false, true) #相当于：update student set sage =30 where sname = &apos;lisi&apos;; 操作截图如下：6、删除数据 1db.student.remove({sname: &apos;chenliu&apos;}) #相当于：delete from student where sname=&apos;chenliu&apos; 操作截图如下： 7、退出shell命令模式输入exit或者Ctrl+C退出shell命令模式 参考MongoDB官方文档： 1https://docs.mongodb.com/master/tutorial/install-mongodb-on-ubuntu/?_ga=2.38644067.1704633038.1522762185-1747967431.1522762185","link":"/2018/08/12/Ubuntu-MongoDB数据库的安装及使用/"},{"title":"docker部署Django","text":"一、安装docker 二、拉取镜像 三、运行镜像，映射端口 四、安装容器内运行环境 五、主机与容器间资源传输 六、容器安装+配置 uwsgi 七、容器安装+ 配置nginx 八、负载均衡 九、Docker提交保存镜像 十、坑点 ####################################################################### 前言 Django部署流程如下 1、docker安装 2、拉取centos镜像或者Ubuntu镜像 看你用哪个 3、使用镜像，run出来一个容器A 4、进入容器A，安装uwsgi，把Django部署在下面 5、在启动脚本中配置开机自启动脚本(这步有点难，需要命令方面需要使用特权) 6、提交容器A成为新镜像，run这个新镜像为容器，项目就部署完成 6、在使用centos镜像或者Ubuntu镜像，run一个容器B，在容器B中安装nginx，配置nginx转发地址即可完成部署 说明 1、在以后项目迁移到任何操作系统下通吃 2、多并发时候，可以通过run多个容器A,在配置nginx即可完成 ####################################################################### 环境： 主机环境：windows 10专业版 一、安装docker Hub.docker.com官网下载 docker for windows 安装完成后，任务栏会 表示已经运行起来，使用注册的账号进行 登录，显示docker is running表示windows环境下已经开始运行 注意：注册账号需要你翻墙才可以，因为那个校验码是需要谷歌的一个api 二、拉取镜像 Docker 在 windows下安装完成后会同时安装一个shell环境 与cmd功能一样 使用 docker images 查看当前有哪些镜像文件 在hub.docker.com上选择好镜像后 使用镜像的 pull 命令就可以进行下载 下载完成就可以使用 docker images看到下载到本地的镜像文件 三、运行镜像，映射端口 启动镜像，在启动镜像的时候可以进行端口映射 启动命令： docker run -d -it -p 805:80 -p 5002:5000 -p 10027:22 -p 20001:8888 -p 8001:8000 –name centos-django –privileged=true -e LANG=zh_CN.utf8 licw1986/centos-django /usr/sbin/init 说明 –privileged=true/usr/sbin/init这两行主要解决容器中使用systemctl去启动服务时出现D_Bus错误的问题 -d: 后台运行-it: 组合使用，创建一个伪终端-p: 端口映射，本机端口：容器端口–name: 容器名，（需要注意命名规范，可自己定义 ，only [a-zA-Z0-9][a-zA-Z0-9_.-]）-e：防止乱码，编码格式最后跟上下载下来的镜像名 镜像启动成功后会返回一串id号，特别长 通过 docker ps –a查看当前docker 中哪些容器正在运行 第一次初始化的时候，使用run，后来都是用start docker start container Id 启动后，在次进入docker exec -it id bash 重启docker restart id 停止docker stop id 删除docker rm id 注意：删除前需要停止镜像 四、安装容器内运行环境 根据所需要的开发项目进行环境搭建，与一般机器配置无异，如果需要图形化界面需要另外安装插件 由于本人需要运行django项目，所以配置的环境主要是 Django==1.8.2mysql-connector-python==8.0.12mysqlclient==1.3.13Pillow==5.2.0protobuf==3.6.1six==1.11.0uWSGI==2.0.17.1nginx:nginx version: nginx/1.12.2 Mysql的坑和uwsgi的坑，请看下面专门描述的坑点 五、主机与容器间资源传输 Windows机器与容器间传输文件，由于是文本界面，无法像图形界面一样使用CV大法，一切靠命令 直接上代码 sudo docker cp txcrm2:/home/log/production.log /system/logs ##仔细看 txcrm2是镜像id ，后面就是文件路径以及复制到宿主机的文件路径 从主机复制到容器sudo docker cp host_path containerID:container_path 从容器复制到主机sudo docker cp containerID:container_path host_path 最后拷贝到本地使用scp 六、容器安装+配置 uwsgi 安装uwsgi确实花了点功夫，都在处理些奇怪的坑，但好在已经配完，遇到的坑在后面进行了描述 配置uwsgi 进入的项目文件夹，新建一个uwsgi.ini的配置文件 启动uswsgi uwsgi –ini uwsgi.ini 这里也有一点小坑，就是你在启动uwsgi的时候，会在当前所在目录去创建uwsgi.pid和uwsgi.log，这个可以能过uwsgi的配置文件去固定一个位置进行保存 Uwsgi 停止 Uwsgi –stop uwsgi.pid 可以通过浏览器直接访问测试uwsgi是否启动成功 七、容器安装+ 配置nginx Nginx安容易多了。直接使用yum install nginx就可以安装成功 Nginx start /etc/nginx/nginx.conf 启动 Nginx stop 停止 Nginx –s reload 重启nginx服务 配置nginx Upstream 主要用来做负载均衡 其中 location 是定义路由，如果使用location = / 是属于精准匹配，不加 = 就相当于正则匹配 proxy_pass 转发服务到哪里， 后面拼接一定要加上http:// 否则也是不成功的，这里要注意，缩进无所谓，能看明白就行，但语法千万不能错，一个单词错就没法启动了（本人粗心大意在这里出了错） 如果出错：可以使用 systemctl status nginx.service查看错误信息 静态文件nginx是无法从项目中获取的。需要 自己创建静态文件夹，项目中使用python manage.py collectstatic 八、负载均衡 这里其实就是在upstream 当中加入多一条server信息，nginx会自动进行选择转发 九、Docker提交保存镜像 1、保存：docker commit 镜像id 自定命名， 保存自定义名时最好保存格式为：账户名/镜像名 方便上传时不需要再更改 2、登录docker login 一般如果在软件中已经登录，这里可以不用登录了，可以当成验证是否成功登录check一下 3、如果是第一次准备上传镜像需要先到 hub.docker.com中登录创建一个镜像仓库 PS:全英文环境，E文不好的朋友可以用chrome的翻译功能，大致没毛病 按照步骤一步步操作就好了 3、上传自定镜像 首先查看一下本地的镜像有哪些 docker push 镜像名：tags 出现进度条，就成功了，等待就可以 十、坑点 1、windows下docker下载，需要注册 2、通过nginx转发时，会丢失掉端口的问题 http://elim.iteye.com/blog/2286952， 在配置nginx时，server内设置： proxy_set_header Host $host， host是不带端口号的，将其改为proxy_set_header Host $host:805，问题得以解决，其中805是docker容器映射到本机的端口 3、安装环境时，mysql-python是不支持python3的，所以需要更换其它方式来进行数据库交互工作（在这个地方折腾了很久） 我在这里使用了mysql官方提供的 mysql-connect-python 和 mysqlclient 还可以安装pymysql 将项目的初始化文件中加入 Import pymysql Pymsql.install_as_MySQLdb() 更改连接方式使用pymsql进行连接 另外最好使用离线包安装mysql 4、安装uwsgi 安装uwsgi的时候，使用pip3 install uwsgi 明明是已经安装完成，显示安装成功，pip freeze也可以看到安装的版本号，仍然提示uwsgi命令无法找到，多次测试之后还是不行。最后去到uwsgi的安装目录下面查看，确实已经安装成功，而且目录下面是可以启动的，为了方便，做了了软链接，测试成功，这里安装uwsgi使用的是离线包安装 5、用docker运行service 时会报错，centos中服务启动和关闭使用systemctl 但是，问题在于使用docker 的时候，systemctl是用不了的，需要在run时加入两条命令，说实话原理我没搞懂，反正把问题解决就行了 加入的命令是： –privileged=true/usr/sbin/init这两行主要解决容器中使用systemctl去启动服务时出现D_Bus错误的问题 重新启动容器就正常了 6、无关docker的坑…… 在项目中生成静态文件时出的错，也提一下 使用python manage.py collectstatic的时候确认要求输入 yes or no，自己傻输入的是y, 然后、、、、就傻了","link":"/2017/01/26/docker部署Django/"},{"title":"docker容器网页远程访问桌面","text":"docker容器网页远程访问桌面前言： 说在前面，目前市面上的docker images界面都是基于xfce的，我们通常安装的Ubuntu都是Unity的，二者安装完后不一样，会对学生学习造成很大困扰，所以在这里介绍一下如何安装基于Unity的镜像 一、主要功能: 通过网页访问docker容器的界面，达到图形化操作 二、专业名词 docker:一个管理容器的东西，类似虚拟机的概念，但是比虚拟机更加快捷方便，多用于部署使用 VNCServer:主要用于启动服务，使用VNCViewer可以直接远程访问 VNCViewer：客户端程序，用于任意的远程连接VNCServer启动的服务 noVNC：主要和VNCServer一起，为VNCServer提供网页访问支持 三、部署 sudo docker run -itd -p 6080:6080 -e PASSWORD=1 -e SUDO=yes –name zc004 land007/ubuntu-unity-novnc 启动后访问以下地址即可远程显示，密码是1。这个1即是vnc连接的1，也是root密码 http://localhost:6080/vnc.html 四、嵌入到项目 五、代码动态创建 1client.create_container(image=&apos;land007/ubuntu-unity-novnc&apos;, name=&apos;zc009&apos;, ports=[6080], command = &quot;/home/ubuntu/startup.sh&quot;, environment={&quot;LANG&quot;: &quot;zh_CN.utf8&quot;,&quot;PASSWORD&quot;:&quot;1&quot;,&quot;SUDO&quot;:&quot;yes&quot;}, hostname=&apos;zc01&apos;, host_config=client.create_host_config(privileged=True, port_bindings={6080:6080})) 1#这个命令在Ubuntu没有 command = &quot;/usr/sbin/init&quot;，但是一定要执行/home/ubuntu/startup.sh，否则容器会挂掉 五、xfce桌面 如果不是太在乎桌面的话，可以在hub.docker.com上搜索consol/ubuntu-xfce-vnc 这个镜像进行使用，这个系列包含了Ubuntu，centos的版本，比较全面 docker run -d -p 5901:5901 -p 6901:6901 consol/centos-xfce-vnc 访问以下地址即可使用 http://localhost:6901/vnc.html","link":"/2018/11/02/docker容器网页远程访问桌面/"},{"title":"mysql_docker_groupby","text":"Mac下mysql5.7 group by报错问题在Mac下执行分组 1select * from tagtable group by categroy order by numid 分组会出现错误 Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column ‘student.student2.id’ which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by mac中安装的mysql默认是没有配置文件的，我们要做的就是 进入数据库查看sqlmodel 1cd /usr/local/mysql/bin./mysql -uroot -pselect @@global.sql_mode; 如果有only_full_group_by这个模式，你继续往下看 先关闭mysql服务 1/usr/local/mysql/support-files/mysql.server stop 如果出错没停止，就从偏好设置里面关掉它 然后将support-files文件夹下my-default.cnf内容添加一行，内容如下： 1sql_mode=&quot;&quot; 保存，然后copy到目录/etc/下面 1sudo cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf 然后启动mysql 1sudo /usr/local/mysql/support-files/mysql.server start 在执行就无错了","link":"/2019/08/25/mysql-docker-groupby/"},{"title":"mysql_Ubuntu","text":"Ubuntu下安装mysql、ssh、jupyter安装mysql** sudo apt-get install mysql-server 输入密码MyNewPass4! sudo apt-get install mysql-client sudo apt-get install libmysqlclient-dev sudo service mysql start sudo netstat -tap |grep mysql mysql -u root -p 输入密码 MyNewPass4! 修改外部访问权限 mysql&gt;use mysql; mysql&gt;update user set host =’%’ where user=’root’ mysql&gt;select host,user from user; mysql&gt;exit mysql下不存在sql_mode问题 重启 sudo service mysql restart 安装ssh sudo apt-get update sudo apt-get install openssh-server sudo service ssh start sudo ps -e |grep ssh 可以查看到sshd已经启动了 jupyter是依赖python环境的 sudo apt-get install python3-pip sudo pip3 install –upgrade pip sudo pip3 install jupyter 测试 jupyter notebook –ip=127.0.0.1 以上是可以的，但是外部无法访问，需要修改其中的配置 jupyter notebook –generate-config 就会在个人用户目录下生成.jupyter jupyter_notebook_config.py vim ~/.jupyter/jupyter_notebook_config.py 修改里面的配置信息,在大约26%位置 #c.NotebookApp.ip= ‘localhost’ 修改以上配置为以下内容 c.NotebookApp.ip=’*’ Ubuntu下启动命令为 jupyter notebook –ip=0.0.0.0 如果要在脚本中运行，还需要增加参数 jupyter notebook –ip=0.0.0.0 –allow-root","link":"/2019/08/25/mysql-Ubuntu/"},{"title":"mysql_docker","text":"docker的centos7.2下安装mysql启动容器docker run -d -it -p 805:80 -p 5002:5000 -p 10027:22 -p 20001:8888 -p 20002:3306 –name centos_master –privileged=true -v /var/run/docker.sock:/var/run/docker.sock -v /zc:/zc -e LANG=zh_CN.utf8 zhangcheng0111/centos7-webssh-dockermanager-ssh /usr/sbin/init 进入容器 docker exec -it centos_master bash yum install -y wget wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum install -y mysql-server systemctl start mysqld.service systemctl status mysqld.service grep 'temporary password' /var/log/mysqld.log 修改**root**密码 获得初始密码后，第一件事就是要重新设置root密码，否则什么事情也做不了，因为MySQL强制要求必须重新设置root密码。 进入mysql数据库 修改密码 mysql -uroot -p ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘MyNewPass4!’; 修改外部访问权限 mysql&gt;use mysql; mysql&gt;update user set host = ‘%’ where user = ‘root’; mysql&gt;select host, user from user; mysql&gt;exit 修改sql_mode模式 vi /etc/my.cnf sql_mode=”” 重启 systemctl restart mysqld.service 如果忘记密码或者找回密码 1.修改配置文件my.cfg[root@localhost ~]# vi /etc/my.cnf找到mysqld在之后添加skip-grant-tables保存退出 2.重启服务service mysqld restart 3.直接登陆mysql而不需要密码mysql 然后修改密码 4.在mysql中输入update mysql.user set password=password(‘root’) where user=’root’;（此时提示ERROR 1054 (42S22): Unknown column ‘password’ in ‘field list’） 5.（这是怎么回事？）原来是mysql数据库下已经没有password这个字段了，password字段改成了authentication_stringupdate mysql.user set authentication_string=password(‘123456′) where user=’root’ ; 6.执行 flush privileges;7.退出mysql ，到/etc/my.cnf中把开始添加的skip-grant-tables去掉8.重启mysql服务","link":"/2017/06/25/mysql-docker/"},{"title":"mysql_win","text":"严格意义上来说mysql8和win10都是大坑，里面有着无数不一样的地方 1：首先去官网下载安装包 下载地址：https://dev.mysql.com/downloads/mysql/ 选择对应5.7.24版本的 2：将解压文件解压到你安装的目录：E:\\mysql-5.7.24-winx64 （我这是放在E盘根目录，不要放在有中文名字和空格的的目录下.例如 3：在mysql-5.7.24-winx64文件夹下面新建一个my.ini文件和一个data文件夹，(注意my.ini后缀名是ini,别弄个my.ini.txt我也为你准备了这个文件，记得下载下来以后解压缩) 注意我这里里面写的路径和版本号你需要修改下 my.ini 不想下载，可以创建my.ini,复制以下内容 my.ini内容： 5：以管理员的身份打开cmd窗口跳转路径到E:\\mysql-8.0.11-winx64\\bin 初始化命令 1./mysqld --initialize --user=mysql --console (记住一定要进行初始化，很多人不进行初始化，就出现了1067错误，怎么弄都搞不定) 初始化完成之后，会生成一个临时密码这里需要注意把临时密码记住（在） 然后在你的数据库保存目录生成一些文件，不用管 这里需要注意使用./mysqld 因为你直接使用mysqld，有可能会找不到 接着就是输入 1mysqld -install 进行服务的添加 输入net start mysql启动服务 输入mysql -u root -p进行登录数据库，这时提示需要密码，然后就是用你上面的密码登录 修改密码语句：ALTER USER root@localhost IDENTIFIED BY ‘123456’; 修改密码为：123456 —————————————————————————————– 接下来大坑就来了 在连接Navicat时候，会出现1251的错误，这个是因为mysql8 之前的版本中加密规则是mysql_native_password,而在mysql7之后,加密规则是caching_sha2_password, 解决问题方法有两种,一种是升级navicat驱动,一种是把mysql用户登录密码加密规则还原成mysql_native_password. ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘123456’ PASSWORD EXPIRE NEVER; #修改加密规则ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’; #更新一下用户的密码FLUSH PRIVILEGES; #刷新权限 —————————————————————————————– 我看好多人拿着命令直接复制，这样是不对的。 ‘root’ 为你自己定义的用户名 ‘localhost’ 指的是用户开放的IP，可以是’localhost’(仅本机访问，相当于127.0.0.1)，可以是具体的’...‘(具体某一IP)，也可以是 ‘%’ (所有IP均可访问) ‘password’ 是你想使用的用户密码 —————————————————————————————– 问题就解决了","link":"/2018/08/25/mysql-win/"},{"title":"mysql忘记密码解决方法","text":"mysql 解决忘记密码问题 mac下mysql 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: 新入了mac pro，安装好mysql后，用终端进入mysql遇到个问题： 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: N 讲道理，我还设密码呢，但是第一次进来就报错，goo了一下大概原因可能是mysql创建的时候给自动分配了密码。 不管分配没分配密码，反正一般的解决方法就是：先跳过验证，再重设密码。 具体步骤： 先关闭MySQL服务；执行 sudo /usr/local/mysql/support-files/mysql.server stop 或者在系统偏好设置中关闭MySQL服务(如果电脑有设置密码的，此处会要求输入计算机密码) 2.去mysql文件夹里设置跳过验证（3步） 先进入mysql文件夹： cd /usr/local/mysql/bin/​​ 设置权限，如果电脑有设置密码（开机和解锁计算机时要求输入的那个密码），在输入此句后会要求输入密码。输入密码后按回车确认 sudo su 跳过验证 ./mysqld_safe --skip-grant-tables &amp;3.开始设置我们自己的新密码 打开一个新的终端，输入 /usr/local/mysql/bin/mysql -u root -p然后会要求输入密码，因为此时根本没有密码，所以直接点确认，显示以下信息表示成功进入mysql(与windows系统一样) Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 20 Server version: 5.7.11 Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&gt; 现在设置新密码，注意要打引号： UPDATE mysql.user SET authentication_string=PASSWORD(&apos;cc77&apos;) where User=&apos;root&apos;;回车确认，显示以下信息表示修改密码成功 Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1然后刷新一下，让上述修改生效： flush privileges;刷新成功会显示以下信息： Query OK, 0 rows affected (0.01 sec)4.重启mysql,用新密码登录，可以登录成功了。但是进行其他mysql操作，会显示： ERROR 1820 (HY000) :You must reset your password using AlTER USER statement before executing this statement. 意思是还要再重设一遍密码，直接输入： SET PASSWORD = PASSWORD(&apos;cc77&apos;);修改成功后，终端会显示： Query OK, 0 rows affected, 1 warning (0.01 sec)至此，修改密码彻底完成，可以做任何相关sql操作了","link":"/2019/07/23/mysql/"},{"title":"python内置函数整理","text":"内置函数就是python给你提供的, 拿来直接用的函数，比如print.，input等。截止到python版本3.6.2 python一共提供了68个内置函数。 1#68个内置函数# abs() dict() help() min() setattr()# all() dir() hex() next() slice() # any() divmod() id() object() sorted() # ascii() enumerate() input() oct() staticmethod() # bin() eval() int() open() str() # bool() exec() isinstance() ord() sum() # bytearray() ﬁlter() issubclass() pow() super() # bytes() ﬂoat() iter() print() tuple() # callable() format() len() property() type() # chr() frozenset() list() range() vars() # classmethod() getattr() locals() repr() zip() # compile() globals() map() reversed() __import__() # complex() hasattr() max() round() # delattr() hash() memoryview() set() 和数字相关 1. 数据类型 bool : 布尔型(True,False) int : 整型(整数) float : 浮点型(小数) complex : 复数 2. 进制转换 bin() 将给的参数转换成二进制 otc() 将给的参数转换成八进制 hex() 将给的参数转换成十六进制 1print(bin(10)) # 二进制:0b1010print(hex(10)) # 十六进制:0xaprint(oct(10)) # 八进制:0o12 3. 数学运算 abs() 返回绝对值 divmode() 返回商和余数 round() 四舍五入 pow(a, b) 求a的b次幂, 如果有三个参数. 则求完次幂后对第三个数取余 sum() 求和 min() 求最小值 max() 求最大值 1print(abs(-2)) # 绝对值:2print(divmod(20,3)) # 求商和余数:(6,2)print(round(4.50)) # 五舍六入:4print(round(4.51)) #5print(pow(10,2,3)) # 如果给了第三个参数. 表示最后取余:1print(sum([1,2,3,4,5,6,7,8,9,10])) # 求和:55print(min(5,3,9,12,7,2)) #求最小值:2print(max(7,3,15,9,4,13)) #求最大值:15 和数据结构相关 1. 序列 （1）列表和元组 list() 将一个可迭代对象转换成列表 tuple() 将一个可迭代对象转换成元组 1print(list((1,2,3,4,5,6))) #[1, 2, 3, 4, 5, 6]print(tuple([1,2,3,4,5,6])) #(1, 2, 3, 4, 5, 6) （2）相关内置函数 reversed() 将一个序列翻转, 返回翻转序列的迭代器 slice() 列表的切片 1lst = &quot;你好啊&quot;it = reversed(lst) # 不会改变原列表. 返回一个迭代器, 设计上的一个规则print(list(it)) #[&apos;啊&apos;, &apos;好&apos;, &apos;你&apos;]lst = [1, 2, 3, 4, 5, 6, 7]print(lst[1:3:1]) #[2,3]s = slice(1, 3, 1) # 切片用的print(lst[s]) #[2,3] （3）字符串 str() 将数据转化成字符串 1print(str(123)+&apos;456&apos;) #123456 format() 与具体数据相关, 用于计算各种小数, 精算等. 1s = &quot;hello world!&quot;print(format(s, &quot;^20&quot;)) #剧中print(format(s, &quot;&lt;20&quot;)) #左对齐print(format(s, &quot;&gt;20&quot;)) #右对齐# hello world! # hello world! # hello world!print(format(3, &apos;b&apos; )) # 二进制:11print(format(97, &apos;c&apos; )) # 转换成unicode字符:aprint(format(11, &apos;d&apos; )) # ⼗进制:11print(format(11, &apos;o&apos; )) # 八进制:13 print(format(11, &apos;x&apos; )) # 十六进制(⼩写字母):bprint(format(11, &apos;X&apos; )) # 十六进制(大写字母):Bprint(format(11, &apos;n&apos; )) # 和d⼀样:11print(format(11)) # 和d⼀样:11print(format(123456789, &apos;e&apos; )) # 科学计数法. 默认保留6位小数:1.234568e+08print(format(123456789, &apos;0.2e&apos; )) # 科学计数法. 保留2位小数(小写):1.23e+08print(format(123456789, &apos;0.2E&apos; )) # 科学计数法. 保留2位小数(大写):1.23E+08print(format(1.23456789, &apos;f&apos; )) # 小数点计数法. 保留6位小数:1.234568print(format(1.23456789, &apos;0.2f&apos; )) # 小数点计数法. 保留2位小数:1.23print(format(1.23456789, &apos;0.10f&apos;)) # 小数点计数法. 保留10位小数:1.2345678900print(format(1.23456789e+3, &apos;F&apos;)) # 小数点计数法. 很大的时候输出INF:1234.567890 bytes() 把字符串转化成bytes类型 1bs = bytes(&quot;今天吃饭了吗&quot;, encoding=&quot;utf-8&quot;)print(bs) #b&apos;\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9\\xe5\\x90\\x83\\xe9\\xa5\\xad\\xe4\\xba\\x86\\xe5\\x90\\x97&apos; bytearray() 返回一个新字节数组. 这个数字的元素是可变的, 并且每个元素的值得范围是[0,256)ret = bytearray(&quot;alex&quot; ,encoding =&apos;utf-8&apos;)print(ret[0]) #97print(ret) #bytearray(b&apos;alex&apos;)ret[0] = 65 #把65的位置A赋值给ret[0]print(str(ret)) #bytearray(b&apos;Alex&apos;) ord() 输入字符找带字符编码的位置 chr() 输入位置数字找出对应的字符 ascii() 是ascii码中的返回该值 不是就返回u 1print(ord(&apos;a&apos;)) # 字母a在编码表中的码位:97print(ord(&apos;中&apos;)) # &apos;中&apos;字在编码表中的位置:20013print(chr(65)) # 已知码位,求字符是什么:Aprint(chr(19999)) #丟for i in range(65536): #打印出0到65535的字符 print(chr(i), end=&quot; &quot;)print(ascii(&quot;@&quot;)) #&apos;@&apos; repr() 返回一个对象的string形式 1s = &quot;今天\\n吃了%s顿\\t饭&quot; % 3print(s)#今天# 吃了3顿 饭print(repr(s)) # 原样输出,过滤掉转义字符 \\n \\t \\r 不管百分号%#&apos;今天\\n吃了3顿\\t饭&apos; 2. 数据集合 字典：dict 创建一个字典 集合：set 创建一个集合 frozenset() 创建一个冻结的集合，冻结的集合不能进行添加和删除操作。 3. 相关内置函数 len() 返回一个对象中的元素的个数 sorted() 对可迭代对象进行排序操作 (lamda) 语法：sorted(Iterable, key=函数(排序规则), reverse=False) Iterable: 可迭代对象 key: 排序规则(排序函数), 在sorted内部会将可迭代对象中的每一个元素传递给这个函数的参数. 根据函数运算的结果进行排序 reverse: 是否是倒叙. True: 倒叙, False: 正序 1lst = [5,7,6,12,1,13,9,18,5]lst.sort() # sort是list里面的一个方法print(lst) #[1, 5, 5, 6, 7, 9, 12, 13, 18]ll = sorted(lst) # 内置函数. 返回给你一个新列表 新列表是被排序的print(ll) #[1, 5, 5, 6, 7, 9, 12, 13, 18]l2 = sorted(lst,reverse=True) #倒序print(l2) #[18, 13, 12, 9, 7, 6, 5, 5, 1] 1#根据字符串长度给列表排序lst = [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;five&apos;, &apos;six&apos;]def f(s): return len(s)l1 = sorted(lst, key=f, )print(l1) #[&apos;one&apos;, &apos;two&apos;, &apos;six&apos;, &apos;four&apos;, &apos;five&apos;, &apos;three&apos;] enumerate() 获取集合的枚举对象 1lst = [&apos;one&apos;,&apos;two&apos;,&apos;three&apos;,&apos;four&apos;,&apos;five&apos;]for index, el in enumerate(lst,1): # 把索引和元素一起获取,索引默认从0开始. 可以更改 print(index) print(el)# 1# one# 2# two# 3# three# 4# four# 5# five all() 可迭代对象中全部是True, 结果才是True any() 可迭代对象中有一个是True, 结果就是True 1print(all([1,&apos;hello&apos;,True,9])) #Trueprint(any([0,0,0,False,1,&apos;good&apos;])) #True zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个元组, 然后返回由这些元组组成的列表. 如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同 1lst1 = [1, 2, 3, 4, 5, 6]lst2 = [&apos;醉乡民谣&apos;, &apos;驴得水&apos;, &apos;放牛班的春天&apos;, &apos;美丽人生&apos;, &apos;辩护人&apos;, &apos;被嫌弃的松子的一生&apos;]lst3 = [&apos;美国&apos;, &apos;中国&apos;, &apos;法国&apos;, &apos;意大利&apos;, &apos;韩国&apos;, &apos;日本&apos;]print(zip(lst1, lst1, lst3)) #&lt;zip object at 0x00000256CA6C7A88&gt;for el in zip(lst1, lst2, lst3): print(el)# (1, &apos;醉乡民谣&apos;, &apos;美国&apos;)# (2, &apos;驴得水&apos;, &apos;中国&apos;)# (3, &apos;放牛班的春天&apos;, &apos;法国&apos;)# (4, &apos;美丽人生&apos;, &apos;意大利&apos;)# (5, &apos;辩护人&apos;, &apos;韩国&apos;)# (6, &apos;被嫌弃的松子的一生&apos;, &apos;日本&apos;) fiter() 过滤 (lamda) 语法：fiter(function. Iterable) function: 用来筛选的函数. 在ﬁlter中会自动的把iterable中的元素传递给function. 然后根据function返回的True或者False来判断是否保留留此项数据 , Iterable: 可迭代对象 1def func(i): # 判断奇数 return i % 2 == 1 lst = [1,2,3,4,5,6,7,8,9]l1 = filter(func, lst) #l1是迭代器print(l1) #&lt;filter object at 0x000001CE3CA98AC8&gt;print(list(l1)) #[1, 3, 5, 7, 9] map() 会根据提供的函数对指定序列列做映射(lamda) 语法 : map(function, iterable) 可以对可迭代对象中的每一个元素进行映射. 分别去执行 function 1def f(i): return ilst = [1,2,3,4,5,6,7,]it = map(f, lst) # 把可迭代对象中的每一个元素传递给前面的函数进行处理. 处理的结果会返回成迭代器print(list(it)) #[1, 2, 3, 4, 5, 6, 7] 和作用域相关 locals() 返回当前作用域中的名字 globals() 返回全局作用域中的名字 1def func(): a = 10 print(locals()) # 当前作用域中的内容 print(globals()) # 全局作用域中的内容 print(&quot;今天内容很多&quot;)func()# {&apos;a&apos;: 10}# {&apos;__name__&apos;: &apos;__main__&apos;, &apos;__doc__&apos;: None, &apos;__package__&apos;: None, &apos;__loader__&apos;: # &lt;_frozen_importlib_external.SourceFileLoader object at 0x0000026F8D566080&gt;, # &apos;__spec__&apos;: None, &apos;__annotations__&apos;: {}, &apos;__builtins__&apos;: &lt;module &apos;builtins&apos; # (built-in)&gt;, &apos;__file__&apos;: &apos;D:/pycharm/练习/week03/new14.py&apos;, &apos;__cached__&apos;: None,# &apos;func&apos;: &lt;function func at 0x0000026F8D6B97B8&gt;}# 今天内容很多 和迭代器/生成器相关 range() 生成数据 next() 迭代器向下执行一次, 内部实际使⽤用了__ next__()⽅方法返回迭代器的下一个项目 iter() 获取迭代器, 内部实际使用的是__ iter__()⽅方法来获取迭代器 1for i in range(15,-1,-5): print(i)# 15# 10# 5# 0lst = [1,2,3,4,5]it = iter(lst) # __iter__()获得迭代器print(it.__next__()) #1print(next(it)) #2 __next__() print(next(it)) #3print(next(it)) #4 字符串类型代码的执行 eval() 执行字符串类型的代码. 并返回最终结果 exec() 执行字符串类型的代码 compile() 将字符串类型的代码编码. 代码对象能够通过exec语句来执行或者eval()进行求值 1s1 = input(&quot;请输入a+b:&quot;) #输入:8+9print(eval(s1)) # 17 可以动态的执行代码. 代码必须有返回值s2 = &quot;for i in range(5): print(i)&quot;a = exec(s2) # exec 执行代码不返回任何内容# 0# 1# 2# 3# 4print(a) #None# 动态执行代码exec(&quot;&quot;&quot;def func(): print(&quot; 我是周杰伦&quot;)&quot;&quot;&quot; )func() #我是周杰伦 1code1 = &quot;for i in range(3): print(i)&quot;com = compile(code1, &quot;&quot;, mode=&quot;exec&quot;) # compile并不会执行你的代码.只是编译exec(com) # 执行编译的结果# 0# 1# 2code2 = &quot;5+6+7&quot;com2 = compile(code2, &quot;&quot;, mode=&quot;eval&quot;)print(eval(com2)) # 18code3 = &quot;name = input(&apos;请输入你的名字:&apos;)&quot; #输入:hellocom3 = compile(code3, &quot;&quot;, mode=&quot;single&quot;)exec(com3)print(name) #hello 输入输出 print() : 打印输出 input() : 获取用户输出的内容 1print(&quot;hello&quot;, &quot;world&quot;, sep=&quot;*&quot;, end=&quot;@&quot;) # sep:打印出的内容用什么连接,end:以什么为结尾#hello*world@ 内存相关 hash() : 获取到对象的哈希值(int, str, bool, tuple). hash算法:(1) 目的是唯一性 (2) dict 查找效率非常高, hash表.用空间换的时间 比较耗费内存 1s = &apos;alex&apos;print(hash(s)) #-168324845050430382lst = [1, 2, 3, 4, 5]print(hash(lst)) #报错,列表是不可哈希的 id() : 获取到对象的内存地址s = &apos;alex&apos;print(id(s)) #2278345368944 文件操作相关 open() : 用于打开一个文件, 创建一个文件句柄 1f = open(&apos;file&apos;,mode=&apos;r&apos;,encoding=&apos;utf-8&apos;)f.read()f.close() 模块相关 __ import__() : 用于动态加载类和函数 1# 让用户输入一个要导入的模块import osname = input(&quot;请输入你要导入的模块:&quot;)__import__(name) # 可以动态导入模块 帮 助 help() : 函数用于查看函数或模块用途的详细说明 1print(help(str)) #查看字符串的用途 调用相关 callable() : 用于检查一个对象是否是可调用的. 如果返回True, object有可能调用失败, 但如果返回False. 那调用绝对不会成功 1a = 10print(callable(a)) #False 变量a不能被调用#def f(): print(&quot;hello&quot;) print(callable(f)) # True 函数是可以被调用的 查看内置属性 dir() : 查看对象的内置属性, 访问的是对象中的dir()方法 1print(dir(tuple)) #查看元组的方法","link":"/2018/10/21/python内置函数整理/"},{"title":"Django序列化器问题","text":"序列化器 12345678class BookInfoSerializer(serializers.Serializer): \"\"\"图书数据序列化器\"\"\" id = serializers.IntegerField(label='ID', read_only=True) btitle = serializers.CharField(label='名称', max_length=20) bpub_date = serializers.DateField(label='发布日期', required=False) bread = serializers.IntegerField(label='阅读量', required=False) bcomment = serializers.IntegerField(label='评论量', required=False) image = serializers.ImageField(label='图片', required=False)# 在这里一的一方定义外键 heroinfo_set = serializers.PrimaryKeyRelatedField(read_only=True) 以上会报错，原因是 在定义外部的时候 1heroinfo_set = serializers.PrimaryKeyRelatedField(read_only=True,many=True) 需要添加many=True Django视图集的使用 视图集的定义 12345from rest_framework import mixinsfrom from rest_framework.viewsets import GenericViewSetclass BookInfoViewSet(mixins.ListModelMixin,mixins.RetrieveModelMixin,GenericViewSet): \"\"\"使用GenericViewSet实现返回列表和单一值\"\"\" # 指定序列化器 serializer_class = BookInfoSerializer # 制定查询集 queryset = BookInfo.objects.all() 视图集的加入 1url(r&apos;^books/$&apos;, new_views.BookInfoViewSet.as_view({&apos;get&apos;:&apos;list&apos;})),url(r&apos;^books/(?P&lt;pk&gt;\\d+)/$&apos;, new_views.BookInfoViewSet.as_view({&apos;get&apos;:&apos;retrieve&apos;})),","link":"/2017/03/25/python/"},{"title":"scrapy-1-快速入门","text":"本教程环境是在Mac下正确安装的，windows下各个操作系统没有测试过 在学习前需要了解scrapy能干什么 1、scrapy的设计全部采用异步请求，能够全面提高爬取抓取速度 2、自动配置了重复请求过滤的操作 3、可以通过命令行参数，默认导出json，csv等格式，Excel需要自己实现 4、通过管道下载文件 注：scrapy创建工程和运行程序，都是需要通过终端来完成的，无法通过可视化界面来创建 一、基本入门 安装 1pip install scrapy 安装有可能会出错，更新一下pip 1pip3 install --upgrade pip 在终端中输入python回车，输入一下内容 1import scrapyscrapy.version_info 以上编译通过不报错即可 创建项目 1scrapy startproject demo demo是你工程的名字,默认创建在你当前终端所在的路径上 为了方便后续开发，可以使用pycharm打开这个工程 spider文件夹下创建爬虫文件bookspider.py 12345class ItcastSpider(scrapy.spiders.Spider): name = 'book' start_urls = ['http://books.toscrape.com/'] def parse(self,response): print(\"收到数据\") 说明其中几点 name是这个爬虫的名字，一个scrapy可以有多个爬虫项目，启动时候也是这个name为标记 start_urls是其实url，默认回调是parse函数，是获得的结果 parse中的response 是获得的html结果 运行爬虫 1scrapy crawl books 运行后，会打印response以下内容简单介绍，后续有实际案例 如何爬取下一页内容 如果继续向下爬取，需要通过在parse中调用yield scrapy.Request(next_url,callback=函数名) 实际上就是通过协程来进行处理的，比如next_url爬取的下一页地址，callback函数名是调用parse则使用parse进行解析，这样就形成了循环爬取每一页了 如何解析内容 css方法可以获得样式的节点，respose.css(myclass) 任何节点都可以使用xpath进行读取，extract_first获得的是第一个内容值 如何保存数据 解析html的数据，比如name和price，组装成字典，通过yield返回，这里后面会用到item进行包装数据的 但是返回到哪里了?实际上是返回到item中进行处理了 如何输出数据 在运行终端命令时候通过增加参数 1scrapy crawl books -o books.csv 通过-o books.csv会自动把通过yield返回的item保存到到books.csv中 以上是通过纯粹终端命令行的方式来运行的 如果要使用pycharm来启动调试，则需要在程序内创建run.py 12from scrapy.cmdline import executeexecute(['scrapy','crawl','books','-o','books.csv'])","link":"/2017/01/26/scrapy-1-快速入门/"},{"title":"python源码解释过程","text":"123456789101112131415161718192021222324252627282930313233343536373839首先你可以观察一下程序的运行过程，具体步骤如下：1、编写py文件 A2、在编写一个py文件 B3、在A中import B2、运行py文件A结果：在运行py文件A的时候，会产生一个B的pyc文件，加速装载，但是A在每次使用的时候，由于只装载一次，所以没有pyc以上是对import解释 整体过程解释1.Python先把代码（.py文件）编译成字节码，交给字节码虚拟机，然后虚拟机一条一条执行字节码指令，从而完成程序的执行。2. 字节码字节码在Python虚拟机程序里对应的是PyCodeObject对象。.pyc文件是字节码在磁盘上的表现形式。3. pyc文件PyCodeObject对象的创建时机是模块加载的时候，即import。Python test.py会对test.py进行编译成字节码并解释执行，但是不会生成test.pyc。如果test.py加载了其他模块，如import util，Python会对util.py进行编译成字节码，生成util.pyc，然后对字节码解释执行。如果想生成test.pyc，我们可以使用Python内置模块py_compile来编译。加载模块时，如果同时存在.py和.pyc，Python会尝试使用.pyc，如果.pyc的编译时间早于.py的修改时间，则重新编译.py并更新.pyc。","link":"/2019/08/26/python源码解释过程/"},{"title":"scrapy-3-数据处理","text":"####itemPipeline数据处理 介绍 启动方式 导出数据 一、介绍 在scrapy中，数据经过spider请求解析后，就需要进行对应模型化处理，在scrapy中itemPipeline负责数据处理 itemPipeline相当于数据处理的过滤器一样，可以配置多个，每一个itemPipline处理完成return后，会进入下一个itemPipeline中，如果不return，则表示放弃这段数据 每一个itemPipeline中，需要实现process_item(self,item,spider):这个方法，在这个方法中进行处理数据 open_spider(self,spider) 一般Spider打开时，回调该方法，通常用于处理数据之前的某些初始化工作，比如连接数据库 close_spider(self,spider)一般Spider关闭时，回调该方法，通常用于处理完成所有数据之后，如关闭数据库 from_crawler(cls,crawler) 一般没啥用，创建ItemPipeline对象时候回调该类方法 二、启动方式 启动方式有2种方式，一种在setting.py进行配置，一种在爬虫文件中配置 方式1 在setting.py中进行配置 1ITEM_PIPELINES={'example.pipelines.PriceConverterPipeline':300} example是工程名字，pipelines是所在文件夹，PriceConverterPipeline自定义的itemPipeline，300是优先级，数值越小优先度越大 多个ItemPipLine的应用，可以使用去重操作，在自定义的ItemPipLine中init初始化时候，初始一个set集合，在每次响应process_item时候对比name，从set中进行对比即可 123456789class DuplicatesPipeline(object): def __init__(self): self.book_set=set() def process_item(self,item,spider): name=item['name'] if name in self.book_set: #数据重复不进行返回即可 self.book_sett.add(name) return item 建立完成DuplicatesPipeline后需要在Setting.py中配置 1ITEM_PIPELINES={'example.pipelines,PriceConverterPipeline':300,'example.pipelines.TestPipeline':400,} 方式2 在Spider文件中可以指定 123Class Spider1(scrapy.Spider): name = 'book1' custom_settings = { 'ITEM_PIPELINES':{'pipelineClass1': 300,'pipelineClass2': 400}, } 这样可以解决多个Spider1时候指定ITEM_PIPELINES的问题，可以为每一个Spider指定数据处理的方式 需要注意的是最后一个ItemPipLine处理完成后，会根据命令行来进行处理后输出到哪里 scrapy crawl books -o books.csv 以上这段命令行中的-o books.csv是写入的文件，就是在最后一个ItemPipeline最后return的数据 四、数据导出 scrapy默认导出数据支持5种方式 JSON JsonItemExporter JSON lines JsonLinesItemExporter CSV CsvItemExporter XML XmlItemExporter Pickle PickleItemExporter Marshal MarsshallItemExporter 前4种是极为常用的文本数据格式，后面2种是Python特有的，scrapy本身没有准备Excel格式数据导出 如何导出？ 方式一 命令行参数 -o -t参数指定导出文件路径和导出格式，通常t可以由o进行推测完成 1scrapy crawl books -o books.csv scrapy crawl 是固定的 books是spider中的name -o是输出的路径 books.csv 没有指定其他路径，就是在当前路径下，没有使用o,但是会根据提供的路径，推测出是csv 以下是指定明确的输出格式 1scrapy crawl books -t json -o books.data 这种数据导出，是依赖什么来进行完成的？ 在配置字典FEED_EXPORTERS中搜索Exporter，FEED_EXPORTERS的内容中搜索Exporter，FEED_EXPORTERS的内容由以下二个字典的内容合并而成 默认配置文件中的FEED_EXPORTERS_BASE 用户配置文件中的FEED_EXPORTERS 前者包含内部支持的数据格式，后者包含用户自定义的导出数据格式，以下是Scrapy源码中定义的FEED_EXPORTERS_BASE，它位于scrapy.settings.default_settings模块： 1FEED_EXPORTERS_BASE={'json':'scrapy.exporters.JsonItemExporter','jsonlines':'scrapy.exporters.JsonLinesItemExporter','jl':'scrapy.exporters.JsonLinesItemExporter','csv':'scrapy.exporters.CsvItemExporter','xml':'scrapy.exporters.XmlItemExporter','marshal':'scrapy.exporters.MarshalItemExporter','pickle':'scrapy.exporters.PickleItemExporter',} 在这里严重不建议修改源码，因为程序最终是要配置在服务器上面的，服务器上面的环境修改源码，可就没那么简单了 用户添加新的导出格式，通常是在配置文件setting.py中定义FEED_EXPORTERS，比如导出Excel 1FEED_EXPORTERS={'excel':'my_project.my_exporters.ExcelItemExporter'} 指定导出文件路径，支持%(name)s和%(time)s这2个特殊变量 %(name)s 会被替换为Spider的名字 %(time)s 会被替换为文件创建时间 假设一个项目爬取的有书籍、游戏信息、新闻3个spider 123scrapy crawl books -o '/export_data/%(name)s/%(time)s.csv'scrapy crawl games -o '/export_data/%(name)s/%(time)s.csv'scrapy crawl news -o '/export_data/%(name)s/%(time)s.csv' 以上是爬取的内容，根据书名存放在export_data文件夹下的对应书名目录下，每个目录下根据时间进行保存 方式二 配置文件方式 在setting.py中设置以下参数 12345FEED_URI='export_data/%(name)s.data' 导出数据的位置FEED_FORMAT='csv' 导出数据的格式FEED_EXPORT_ENCODING='gbk' 导出数据的编码FEED_EXPORT_FIELDS=['name','author','price'] 导出数据的字段FEED_EXPORTERS={'excel':'my_project.my_exporters.ExcelItemExporter'} 新添加导出的格式 如何实现一个自定义的导出的Excel类 1、继承BaseItemExporter 其中有3个方法 export_item(self,item) 负责导出爬取的每一项数据，参数item为一项爬取的数据，每一个子类必须实现该方法 start_exporting(self) 在导出开始时被调用，可在该方法执行某些初始化工作 finish_exporting(self) 在导出完成时候被调用，可在该方法中执行某些清理工作 以下实现一个导出Excel,注意这里使用的是xlwt，这个库最多只能在一个sheet中插入65535行数据，如果要插入更多数据，需要切换为openxl来进行 12345678910111213141516171819from scrapy.exporters import BaseItemExporter impoort xlwt class ExcelItemExporter(BaseItemExporter): def __init__(self,file,**kwargs): self._configure(kwargs) self.file=file self.wbook=xlwt.Workbook() self.wsheet=self.wbook.add_sheet('scrapy') self.row=0 def finish_exporting(self): self.wbook.save(self.file) def export_item(self,item): fields=self._get_serialized_fields(item) for col,v in enumerate(x for _,x in fields): self.wsheet.write(self.row,col,v) self.row +=1 编写完成后再setting.py中添加这个类 1FEED_EXPORTERS={'excel':'my_project.my_exporters.ExcelItemExporter'} 之后终端命令行 1scrapy crawl books -t excel -o books.xls 这样就导出excel了，当然你可以封装一个Excel导出类，这样可以更加通用，在web框架下，爬虫框架下都可以使用","link":"/2017/03/02/scrapy-3-数据处理/"},{"title":"scrapy-2-数据解析","text":"1、Request和Respose对象 2、Selector对象 3、xpath使用 4、css选择器 一、Request和Respose对象 1.1 Request对象 Request对象是要请求一个连接所使用的对象 1Request(url[,call,method=‘GET’,headers,body,cookies,meta,encoding=‘utf-8’,priority=0,dont_filter=False,errback]) Url是请求地址 callback是页面回调地址 method是请求方法，get或者post headers是请求头 body是html正文 cookies字典类型 Meta字典类型 encoding是编码格式 priority优先级 0是最高级 dont_filter=False对重复地址自动过滤，如果设置为True会强制请求，适合重复地址随时间变化的 errback请求错误 1.2 Respose对象 Respose对象是当发起请求后，返回的对象，比如下列代码中parse参数中的response 12345class BookSpider(scrapy.spiders.Spider): name = 'book' start_urls = ['http://books.toscrape.com/'] def parse(self,response): print(\"收到数据\") Url 响应地址 status 状态码 headers 获得响应头 Body 响应的正文 bytes类型 Text 文本形式str类型，是有body解码获得的， 实现方式response.text=respose.body.decode(response.encoding) 1.3 start_request(self)方法 当start_urls这个数组无法满足你，你就需要自定义这个start_urls，通过重写start_request的方法来完成 12345678910class demoSpider(RedisSpider): name = \"demospider\" redis_key = 'demospider:start_urls' start_urls = ['http://www.example.com'] def start_requests(self): pages=[] for i in range(1,10): url='http://www.mmonly.cc/mmtp/xgmn/126808_%s.html' % i page=scrapy.Request(url) pages.append(page) return pages 注意:这个start_request只会调用一次,并且在重写了start_request后，start_urls就无效了 二、Selector对象 Html转换为对象，转换方法有很多种 BeautifulSoup lxml 在scrapy中自带selector 可以对html的字符串进行格式 比如text是我们的html字符串 导入包 123from scrapy.selector import Selectortext=\"&lt;html&gt;&lt;body&gt;今天天气很好&lt;/body&gt;&lt;/html&gt;\"selector = Selector(text=text) 这样就可以对selector使用xpath和css方法读取数据了 //h1 选中文档内所有的h1标签 text() 获得text属性值 .//li/b/text() 获得当前标签下的所有li下的b标签的text 提取内容 extract、re 这2个返回数组内容 extract_first、re_first 这2个返回第一个标签内容 具体实例 12#获得当前标签下的h1下的text内容response.xpath('.//h1/text()').extrat() 注：Selector主要是在后面使用requests对获得html解析的时候使用的，单纯使用scrapy的时候，是不需要去手动创建Selector 三、xpath基本使用 3.1 xpath语法 123456789/ 选中文档的根 . 选中当前节点 .. 选中当前节点的父节点ELEMENT 选中子节点的所有ELEMENT元素节点//ELEMENT 选中后代节点中所有ELEMENT元素节点* 选中所有元素子节点 不常用text()选中所有文本子节点@ATTR 选中所有属性节点@* 选中所有属性节点 不常用[谓语] 谓语用来查找某个特定的节点或者包含某个特定值的节点 使用示例 12345678910111213141516/body/div/a 选取body下的div下的a标签//a 基于当前节点下提取所有a标签/body//img 在body下的所有img标签//a/text() 获得所有a标签下的文字节点/body/div//* 选中div下的所有后代元素，一般结合后续筛选,不会作为单独使用//div/*/img 获得div孙节点下所有的img标签//img/@src 选中所有img的src属性//@href 选中文档中所有的ATTR属性//a[1]/img/@* 获得第一个a下的img的所有属性. 用来描述当前节点，也就是表示一个相对路径.. 用来获得上一层节点的路径//img/.. 获得所有img，读取img上一层的节点//a[last()] 获得所有的a标签，选中最后一个//a[position()&lt;=3] 使用position() 选中前三个//div[@id] 选中所有包含id属性的div//div[@id=images] 选择所有id属性，且值为images的div 注意:/和//的区别 在于/是基于当前节点下，不会继续向下穿透其他节点，而//会向下穿透获得所有这个节点 3.2 XPath常用函数 string 12sel.xpath('/body/a//text()') 获得的是每个a标签内容sel.xpath('string(/body/a)') string是把每个a标签内容进行拼接 四、CSS选择器 12345678910111213141516* 选中所有元素p 选中p元素div,p 选中div和p元素div p 选中div后代元素中的p元素div&gt;p 选中div子元素中的p元素p+div 选中p标签的兄弟元素中的div元素.info 选中class属性包含info的元素#main 选中id属性为main的元素[href] 选中属性为href的元素[href='www.baidu.com'] 选中属性为href元素，并且值为www.baidu.com的元素[href~='baidu'] 选中属性为href元素，并且值包含为baidu的元素p::text 选中p元素的文本节点a:nth-child(1) 选中a元素，且该元素必须是其父元素的第1个子元素a:nth-last-child(2) 选中a元素，且该元素必须是其父元素的倒数第2个子元素a:first-child(1) 选中a元素，且该元素必须是其父元素的第1个子元素a:last-child(2) 选中a元素，且该元素必须是其父元素的倒数第2个子元素 使用示例 12#获得当前li标签下的text response.css('li::text').extrat() 在下一章节介绍数据解析完成后，如何处理","link":"/2017/02/27/scrapy-2-数据解析/"},{"title":"scrapy-4-链接提取器","text":"链接提取器，设置规则后，进行批量提取所有连接，主要用于是在CrawlSpider模板中使用的居多 比如提取某个网站上所有数据 使用LinkExtractor需要添加相关头文件 1from scrapy.linkextractors import LinkExtractor 以下代码进行整站爬取。通过Rule+LinkExtractor结合进行提取了所有页面的数据，CrawlSpider想进行整站所有a标签提取，但是Rule+LinkExtractor进行了限制，筛选出符合我规则的数据 1234567891011import scrapyfrom scrapy.spiders import *from scrapy.linkextractors import LinkExtractorclass ItcastSpider(scrapy.spiders.CrawlSpider): name = 'book' start_urls = ['http://books.toscrape.com/'] img_urls = [] rules = (Rule(LinkExtractor(allow=('http://books.toscrape.com/catalogue/page-\\d{1,6}'+'.html',)), callback='parse_item', follow=True),) def parse_item(self,response): print(\"收到数据\") 注意：连接提取器中的里面的逗号是不能少的，因为Rule接受的是一个可迭代的对象元组 这个方法比较适合对整站数据的规则爬取，不需要自己设置反复爬取的地址了 LinkExtractor相关参数说明 allow 接收一个正则表达式的地址，用于过滤出符合条件的地址 deny 与allow相反，接收一个正则表达式，排除符合条件的地址 allow_domains 接收一个域名或一个域名列表，提取到指定域的链接 12345from scrapy.linkextractors import LinkExtractordomains=['github.com','baidu.com']le = LinkExtractor(allow_domains=domains)links = le.extract_links(response)[link.url for link in links] 其中response是获得请求后的结果 deny_domains 与allow_domains相反，排除指定域的连接 restruct_xpaths 接收一个XPath表达式 提取指定页面下的元素下的所有链接 1234from scrapy.linkextractors import LinkExtractorle = LinkExtractor(restruct_xpaths='//div[@id=\"top\"]')links = le.extract_links(response)[link.url for link in links] 其中response是获得请求后的结果 restruct_css 接收一个css选择器或者选择器列表 提取指定页面下下的链接 1234from scrapy.linkextractors import LinkExtractorle = LinkExtractor(restruct_css='div#bottom\"]')links = le.extract_links(response)[link.url for link in links] 提取JavaScript文件的连接 12345from scrapy.linkextractors import LinkExtractorle = LinkExtractor(tags='script',attrs='src')links = le.extract_links(response)[link.url for link in links]process_value 接收一个函数，用于处理连接地址，如果返回none则抛弃这个地址","link":"/2017/04/05/scrapy-4-链接提取器/"},{"title":"scrapy-5-文件图片下载","text":"在scrapy中提供了2种下载通道 FilesPipeline 提供文件下载 ImagesPipline 提供图片的下载，额外提供缩略图 这2个ItemPipline作为特殊的下载器，用户使用时，只需要通过item的一个特殊字段，将要下载文件或图片url进行赋值，他们就会自动将文件或者图片下载本地。并将下载结果信息存入到item的另一个特殊字段中 FilePipline的使用 导入路径 scrapy.pipelines.files.FilesPipeline Item字段 file_urls,files 下载目录 FILES_STORE 请看一下前端页面 12345678&lt;html&gt; &lt;body&gt; &lt;a href='/book/sg.pdf'&gt;下载三国演义&lt;/a&gt; &lt;a href='/book/shz.pdf'&gt;下载水浒传&lt;/a&gt; &lt;a href='/book/hlm.pdf'&gt;下载红楼梦&lt;/a&gt; &lt;a href='/book/xyj.pdf'&gt;下载西游记&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 使用FilePipLine通常置于其他ItemPipline之前 1ITEM_PIPELINES={'scrapy.pipelines.files.FilesPipeline':1} 在配置文件setting.py中使用FILES_STORE指定文件下载目录 1FILES_STORE='/home/zhangcheng/Download/scrapy' 在Spider解析一个包含文件下载链接的页面时，将所有需要下载的文件的url地址收集到一个列表，赋给item的file_urls字段(item[‘file_urls’]) FilesPipeline在处理每一项item时，会读取item[‘file_urls’],对其中每一个url进行下载 12345678910class DownloadBookSpider(scrapy.Spider): def parse(response): item={} #下载列表 item['file_urls']=[] for url in response.xpath('//a/@href').extract(): download_url=response.urljoin(url) #将url填入下载列表 item['file_urls'].append(download_url) yield item 当FilesPipeline下载完item[‘file_urls’]中的所有文件后，会将各文件的下载结果信息收集到另一个列表，赋给item的files字段(item[‘files’])，下载结果包括以下内容 Path 文件下载到本地的路径 Checksum 文件的校验和 url 文件的url地址 ImagesPipeline使用 导入路径 scrapy.pipelines.images.ImagesPipeline Item字段 image_urls,images 下载目录 IMAGES_STORE ImagesPipeline是在FilesPipeline基础上针对图片图片增加了一些特有功能 为图片生成缩略图 开启该功能，需要在setting中设置IMAGES_THUMBS 是一个字典，每一项的值是缩略图的尺寸 1IMAGES_THUMBS={ 'small':(50,50), 'big':(270,270),} 开启该功能后，下载一张图片，本地会出现三张图片，分别在full、thumbs/small下 thumbs/big下 过滤掉尺寸过小的图片 在setting.py中设置IMAGES_MIN_WIDTH和IMAGES_MIN_HEIGHT 1IMAGES_MIN_WIDTH=110IMAGES_MIN_HEIGHT=110 开启该功能后，如果下载了一张105200的图片，该图片就会被抛弃掉，因为宽度不符合 文件下载例子 爬取matplotlib网站源码文件 html解析下载文件 创建项目 1scrapy startproject matplotlib_examples 到项目目录下 1cd matplotlib_examples/ 爬虫模板 1scrapy genspider examples matplotlib.org 编辑items.py增加模型 123class ExampleItem(scrapy.Item): file_urls=scrapy.Field() files=scrapy.Field() 编辑examples.py实现爬虫逻辑 12345678910111213141516171819202122import scrapy from scrapy.linkextractors import LinkExtractor from matplotlib_examples.items import ExampleItem class ExamplesSpider(scrapy.Spider): name = 'examples' allowed_domains = ['matplotlib.org'] start_urls = ['http://matplotlib.org/examples/index.html'] def parse(self, response): le =LinkExtractor(restrict_css='div.toctree-wrapper.compound',deny='/index.html$') for link in le.extract_links(response): yield scrapy.Request(link.url,callback=self.parse_example) def parse_example(self,response): href = response.css('a.reference.external::attr(href)').extract_first() url =response.urljoin(href) example=ExampleItem() print(url) example['file_urls']=[url] return example 在setting.py中增加下载模块，下载模块会自动寻找file_urls字段 1234ITEM_PIPELINES={ 'scrapy.pipelines.files.FilesPipeline':1 } FILES_STORE='examples_src' 增加run.py 123#coding:utf8 from scrapy.cmdline import execute execute('scrapy crawl examples -o examples.json'.split()) 运行run.py 最后产生一个examples.json文件和一个examples_src文件夹，examples.json文件保存抓取下的数据,文件夹中保存的下载后的文件 但是下载的内容，会全部保存在full这个我们指定的文件夹下，但是文件名称为一段很长的sha散列值的文件，这是为了防止文件覆盖，但是这样很不方便阅读 我们期望每个文件都存入我们指定的地方，实际上FilesPipline里面的file_path方法决定了文件名，所以就需要我们继承FilesPipline重写file_path的方法 12345678from urllib.parse import urlparse from os.path import basename,dirname,join from scrapy.pipelines.files import FilesPipeline class MyFilePipeline(FilesPipeline): def file_path(self, request, response=None, info=None): path=urlparse(request.url).path return join(basename(dirname(path)),basename(path)) 在setting.py中添加的 1ITEM_PIPELINES={ # 'scrapy.pipelines.files.FilesPipeline':1 'toscrape_book.pipelines.MyFilesPipeline': 300, } 重新运行后，则展示了对应的文件在对应的文件夹下 此例子可以应用在下载对应的组图中 360图片下载项目 json+下载项目 1scrapy startproject so_image 1cd so_image 1scrapy genspider images image.so.com 使用pycharm打开 在setting.py中添加ImagesPipeline 123ITEM_PIPELINES={ 'scrapy.pipelines.images.ImagesPipeline':1 } IMAGES_STORE='download_images' 实现imagesSpider.py 123456789101112131415161718192021import scrapy import json from scrapy import Request class ImagesSpider(scrapy.Spider): BASE_URL='http://image.so.com/zj?ch=art&amp;sn=%s&amp;listtype=new&amp;temp=1' #限制最大下载量，防止磁盘用量过大 MAX_DOWNLOAD_NUM=1000 name = 'images' allowed_domains = ['image.so.com'] start_urls = [BASE_URL%0] def parse(self, response): #使用json模块解析响应结果 infos= json.loads(response.body.decode('utf-8')) #提取所有的图片下载url到一个列表，赋值给item的image_urls字段 yield {'image_urls':[info['qhimg_url'] for info in infos['list']]} #如count字段大于0，并且下载数据不足MAX_DOWNLOAD_NUM,继续获取下一页图片信息 self.start_index+=infos['count'] if infos['count'] &gt;0 and self.start_index&lt;self.MAX_DOWNLOAD_NUM: yield Request(self.BASE_URL%self.start_index) 在setting.py中设置 1ROBOTSTXT_OBEY = False 创建运行run.py 12from scrapy.cmdline import execute execute(['scrapy','crawl','images']) 查看结果，可以看到下载的图片","link":"/2017/05/03/scrapy-5-文件图片下载/"},{"title":"爬虫+selenium","text":"爬虫使用selenium进行爬取，借助chrome进行调试， 1pip install selenium 代码测试 1from selenium import webdriver browser = webdriver.Chrome()browser.get(&apos;http://www.baidu.com/&apos;) 你会欣喜若狂的运行一下，如果你是初次使用，会报错，需要下载一个webdriver的文件，这个文件要放在chrome浏览器下，最坑的是这个文件需要对应你的浏览器版本才可以 可以访问以下地址查看对应 https://blog.csdn.net/huilan_same/article/details/51896672 1、windows下找chrome的安装位置 2、mac下找chrome.app 右键显示包内容,实际上这个文件chromedriver 在最新的mac上，需要在/usr/local/bin下也弄一份同样的文件，这样还不够，还需要赋值权限chmod 777 /xxxx/xxxx/xxx/chromedriver才可以 3、linux下找到安装位置 以下是对应的chrome的工具，需要根据版本选择对应的工具 http://chromedriver.storage.googleapis.com/index.html 版本编号从2.0-2.38之间，截稿前最新版本是2.33,但是需要注意不是最新的就是能用的，要找到你对应，怎么找对应的 点击每个版本里面，都有一个notes.txt 123456----------ChromeDriver v2.38 (2018-04-17)----------Supports Chrome v65-67----------ChromeDriver v2.37 (2018-03-16)----------Supports Chrome v64-66----------ChromeDriver v2.36 (2018-03-02)----------Supports Chrome v63-65 那个Supports Chrome v65-67就是对应的Chrome的版本，比如你的Chrome版本是64版本，就一定要使用2.37版本或者2.36版本，不可以使用2.38版本 下载后，把下载的文件放入到chrome安装路径下即可(Mac需要在应用程序上右键显示包内容下) 最后运行你的程序即可，就可以不报错了 最后奉上老司机福利，能不能看懂就看你自己了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#coding=utf-8import osimport requestsimport urllib.parsefrom lxml import etreefrom selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport multiprocessing xp_href = \"//div[@class='item']//a/@href|//div[@class='item ']//a/@href\"xp_video_reg = \"//video/@src\" class SpiderYellow(object): def __init__(self): #创建保存文件夹 self.downloadDir = \"./movies\" if os.path.exists(self.downloadDir)==False: os.mkdir(self.downloadDir) #保存此次下载的文件信息 self.downloadInfo = {} #想爬取的个数 self.max_count = 0 #发现的连接名称 self.setHref= set([]) #下载的队列 self.htmlQueue = [] #从连接中获得的视频 self.setUrl = set([]) #谷歌chromeDriver设置请求头 ，并打开无界面浏览器加载动态js网页 chrome_options = Options() chrome_options.add_argument('--headless') chrome_options.add_argument('--disable-gpu') chrome_options.add_argument( 'User-Agent=\"Mozilla/5.0 (iPod; U; CPU iPhone OS 2_1 like Mac OS X; ja-jp) AppleWebKit/525.18.1 (KHTML, like Gecko) Version/3.1.1 Mobile/5F137 Safari/525.20\"') self.driver = webdriver.Chrome(chrome_options=chrome_options) #设置requests请求头 self.headers = {\"User-Agent\":\"Mozilla/5.0 (iPod; U; CPU iPhone OS 2_1 like Mac OS X; ja-jp) AppleWebKit/525.18.1 (KHTML, like Gecko) Version/3.1.1 Mobile/5F137 Safari/525.20\"} #保存的视频信息 def save_video(self,src): print(\"saving... ==&gt;\",src) self.setUrl.add(src) #从第一个界面中获得视频链接地址 def find_videoInfo(self): while len(self.setUrl) &lt; self.max_count: url = self.htmlQueue.pop() if url: print('analysis:', url) html = self.downloadHtml(url) self.analysisHtml(html) def downloadHtml(self,url): self.driver.get(url) return self.driver.page_source def analysisHtml(self,html): tree = etree.HTML(html) # 查找视频信息 videoInfo = tree.xpath(xp_video_reg) # 保存视频信息 for video in videoInfo: self.save_video(video) # 查找新的非重复本域名标签 hrefInfo = tree.xpath(xp_href) for href in hrefInfo: href = urllib.parse.urljoin(self.seedurl, href) if href not in self.setHref: self.setHref.add(href) self.htmlQueue.append(href) def find_video_src(self,url): pass def _download(self,url): print(\"start write to file\",url) r = requests.get(url,headers=self.headers,stream= True) with open(os.path.join(self.downloadDir,url.split('/')[-2]),'wb') as f: for chunk in r.iter_content(chunk_size=1024): f.write(chunk) print(\"download success\",url) def mutiDownload(self,urls): pool = multiprocessing.Pool(10) for url in urls: pool.apply_async(self._download(url)) pool.close() pool.join() def start(self,seed_url,maxCount): self.seedurl = seed_url self.max_count = maxCount self.htmlQueue.append(seed_url) self.find_videoInfo() self.mutiDownload(self.setUrl)s = SpiderYellow()s.start(\"http://dyz44.com/latest-updates/\",3)","link":"/2018/12/03/selenium安装与使用/"},{"title":"关于爬虫的几种常见的反爬","text":"1.基于useragent反爬思想：服务器后台对访问的User_Agent进行统计，单位时间内同一User_Agent访问的次数超过特定的阀值，则会被不同程度的封禁IP，从而造成无法进行爬虫的状况。 我提供给大家两种方案， 方案一： 将常见的User-Agent封装到一个.py文件中，命名为useragent.py 12345# 列表存储不同的User-Agent,这里仅示例三个,详情可百度百度搜索,链接在代码下方ua_list = [&apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0&apos;, &apos;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)&apos; ] 他人为我们总结的常用User-Agent获取链接：https://www.cnblogs.com/zrmw/p/9332801.html 在爬虫过程中导入useragent.py文件，随机选择ua_list中的user-agent。代码示例： 123456789import random# 导入自己定义的useragents.py中的ua_listfrom .useragents import ua_list#随机获取User-Agentheaders = {&apos;User-Agent&apos;:random.choice(ua_list)}req = request.Request( url=&apos;https://...&apos;, headers=headers ) 方法二：Python中加载fake_useragent库，随机生成User-Agent添加到headers中。代码示例： 1234from fake_useragent import UserAgent# 测试fake-useragentua = UserAgent()print(ua.random) ####2.基于IP反爬机制 思想：后台服务器对访问进行统计，单位时间内同一IP访问的次数超过一个特定的值（阀值），就会不同程度的禁封IP，导致无法进行爬虫操作。 解决方案使用不同的IP地址进行访问，设置一定的访问时滞，例：random.sleep(3)。本文分享给大家一种常见的方法，基于西刺代理购买的专业代理构建可用代理池。经常爬虫的伙伴应该对西刺代理并不陌生，可以免费或花钱购买可用的IP地址，但是怎么说呢，免费的很多都不能用，（其实收费的也有很多不能用，所以感觉有点小坑。）西刺代理网址：https://www.xicidaili.com/ 3.动态抓包思想：当我们进入某个网页时，我们想通过查看源代码解析页面结构，看到的内容却不是我们网页显示的内容；或者，在我们浏览网站时网页结构显示不全，只有滑动鼠标时才能将剩余的信息显示出来，我们就需要靠手动抓包来解析不同。","link":"/2019/12/24/关于爬虫的几种常见的反爬/"},{"title":"感-《终身幼儿园》","text":"少儿编程圣经：感 -《终身幼儿园》最近在空闲中翻看了一本所谓少儿编程语言行业的圣经之一《终身幼儿园》。作者Mitchel Resnick(米切尔·雷斯尼克) 是MIT媒体实验室的教授，他开发了儿童编程主流软件Scratch，同时和乐高联手开发出Wedo等编程必备品。刚开始觉得也很平常觉得没什么可以吸取的，但是读着读着 就发发现亮点了。 关于这本书的一些总结和思考： 清华大学校长陈吉宁对作者说的话： 中国的教育制度侧重于培养遵守规则和指示的学生，他称这些学生为“A型学生”，这些学生缺乏创造性和创新精神，而这是在未来社会中取得成功所必备的。陈校长认为，中国需要一种新型的学生，他称为“X型学生”，这些学生愿意承担风险，勇于尝试新鲜事物，乐于提出自己的问题，而不是简单地解决教科书里的问题。正是“X型学生”，将提出创造性的想法，改变未来社会。 其实我觉得这个理念对现在的教育培优可能价值不大，但是在课外辅导中，可以往这个方向靠靠。 在正文中，最有价值的内容可以用“1 + 4”表示。 1 是指一个核心理念。 4 是指在这个理念的指导下，作者提出的 4P 学习法。 从这个脉络中可以看出来，作者是从理念到具体实践方法，提出了一整套可行的框架。在这个框架下，少儿编程将成为培养孩子创造力的利器。 一. 1 个核心理念这是作者的核心理念。 为什么要像在幼儿园里一样的学习呢? 因为在小学、中学的时候，主要学习方式是老师在课堂上面讲课，学生在下面被动接受，很少会有课堂讨论。作者称这种方式为广播模式。 而在幼儿园里，是通过孩子自己动手探索、做游戏、玩来学习的，比如过家家，堆积木，玩橡皮泥。在这个过程中，孩子能创造新事物，发展创造力。作者称这种学习方式为互动学习。 在这种学习过程中，孩子能够反复地想象、创造、游戏、分享和反思，从而提升创造性思维能力。作者的 cratch 编程语言就是在这个理念下设计的。 二. 4P 学习法书本中的 项目。 孩子们可以在网站上新建一个项目，然后在这个项目中实现自己的想法（比如一个小游戏）。项目可以发布出来，让别人试用，最后根据实际反馈进行修改和迭代。在这个过程中，很多互不相识的孩子也可以一起合作完成这个项目。 热情。 在完成项目的时候，兴趣是第一推动力。孩子可以选择自己感兴趣的项目（因为 Scratch 能做的事很多，比如小游戏、音乐、动画等）。在学习上，对孩子最好的奖励是他们自己的成就感，而不是外部的表扬或者物质奖励。 同伴。 在巴西有一种桑巴舞校模式。到了一定的时候，大人，小孩、专家和新手都会聚集在一起，共同合作创作出具有当地特色的舞蹈和歌曲。这种模式在少儿编程社区中也一样，孩子们的编程水平虽然有高有低，但是却能够很好的合作。 游戏。 孩子能玩的游戏有很多。在有的游戏中，孩子能够自己决定做什么，以及怎么做，比如玩积木，这种是构造性游戏。还有的游戏，孩子只能不断地升级打怪，或者是相互 pk，比如打弹珠。而前者更能发展孩子的创造力。 在玩构造性游戏的过程中，孩子需要不断地修修补补，也有可能经常犯错，但是这些都有助于孩子创造新的事物。 读完了这本书，发现书中的 4P 学习法是在在线社区的基础之上，但是国内的少儿编程环境还没有那么成熟，没有办法运作起来。但是在线下，这套方法论是可以借鉴的。比如让几个孩子一起用 Scratch 做小游戏， 然后比一比谁做的小游戏更好玩，同时在玩的过程中，可以不断地改进这些游戏。这样，不但孩子们多了一个功能强大的玩具，而且无形中也发展了创新能力。 对教研有什么用 作为资深的教研来讲做一节课是一件如同吃饭一样简单的事情，而能做出来孩子们都能上的很嗨的课也是如同上厕所一样顺其自然。但是面面都能满足的课是什么样的课呢，或则说”面面“都有哪些”面面“。相信每一个教研老师都有自己的答案，或则都想知道一个标准的答案。这是我最近一段时间最重要的事情了，我也快接近胜利了。 现在的课程大部分都是脚手架偏多，脚手架的设置要有策略，课设时仍然还要给出孩子自己的发挥空间，”open“和”个性化“更容易激发孩子们兴趣…….. (不是本人只能观看到此)","link":"/2020/03/09/感-《终身幼儿园》/"},{"title":"短信服务与邮件服务","text":"短信和邮件是开发过程中必不可少的工具 短信服务 短信服务常用于通知提醒、验证码、营销推广、服务器报警、短信轰炸机等。。。 短信服务需要借助第三方服务商来完成，不同的服务商，短信到达率是不一样的，这里使用云片来完成短信服务 邮件服务 使用自带的邮件服务即可 发送短信 12345678910111213141516171819202122232425#https://www.yunpian.com/api/sms.html#c8 api说明#https://www.yunpian.com/dashboard/domestic/register#!/domestic/register/tpl 模板签名#账号13811928431 密码zc85011import urllibimport urllib2from urllib import quote#短信模板【xxx】#name#您好 现在室外温度:#wendu# 室外湿度:#shidu##参数说明 name 发送短信模板的姓名 wendu 温度 shidu 湿度 iphone 电话号码def sendMsg(name,wendu,shidu,iphone):#验证码模板#test_data = {'apikey':'de83447f8d0f16bff95a2c151803addf','mobile':'13811928431','text':'【xxx】您的验证码是1234'}#quote 是转换urlencode字符串的 tplvalue = quote(\"#name#\") + \"=\" + quote(name) + \"&amp;\" + quote(\"#wendu#\") + \"=\" + quote(wendu)+\"&amp;\" + quote(\"#shidu#\") + \"=\" + quote(shidu); #'tpl_id':'2138996'固定的 test_data = {'apikey':'de83447f8d0f16bff95a2c151803addf','mobile':iphone,'tpl_id':'2138996','tpl_value':tplvalue} test_data_urlencode = urllib.urlencode(test_data) requrl = \"https://sms.yunpian.com/v1/sms/tpl_send.json\" req = urllib2.Request(url = requrl,data =test_data_urlencode) print req res_data = urllib2.urlopen(req) res = res_data.read() print res #https://www.yunpian.com/api/sms.html#调用sendMsg('张同学','34','12','138119283') 邮件服务 1234567891011121314151617181920212223import smtplibfrom email.mime.text import MIMETextfrom email.header import Header# 第三方 SMTP 服务from email.utils import formataddrdef sendMessage(subject,content): mail_host = \"smtp.itcast.cn\" # 设置服务器 mail_user = \"zhangcheng@itcast.cn\" # 用户名 mail_pass = \"z850311\" # 口令 sender = 'z@163.com'#好像什么都行 receivers = ['z@163.com'] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱 #组装数据 str= '大家好\\n我是逗比机器人，在群里有人@了我们，提出了问题' message = MIMEText(str, 'plain', 'utf-8') message['From'] = formataddr([\"MyQQRot\", \"zz@163.com\"]) #拼接多人 message['To'] = ','.join(receivers) message['Subject'] = Header(subject, 'utf-8') smtpObj = smtplib.SMTP() smtpObj.connect(mail_host, 25) # 25 为 SMTP 端口号 smtpObj.login(mail_user, mail_pass) smtpObj.sendmail(sender, receivers, message.as_string()) print(\"邮件发送成功\") 发送邮件 1sendMessage(&apos;邮件主题&apos;,&apos;邮件测试&apos;) 邮件服务 2（3行代码搞定） 话不多说上来先看一下代码 1234import yagmailyag = yagmail.SMTP(user='user@163.com',password='1234',host='smtp.163.com') #注意这个password是你授权密码contents = ['这里是内容']yag.send('target@qq.com','主题：这是一个主题',contents) yagmail.SMTP()参数 yagmail.SMTP()里面的几个重要参数！user和password顾名思义啦，就是你自己的邮箱账号和密码但是这个问题就来了，我们平时登陆邮箱一般账号密码登陆有些时候好像是要打验证码的呀，这样python就传了个账号和密码真的能把邮件发出去吗？然鹅此密码非彼密码，用的密码是邮箱中的授权码，就是专门授权给机器登陆的密码。一般邮箱中，进入设置，然后POP3/SMTP/IMAP设置，打开服务就可以设置授权码了。 send（）参数 args.to 是收件人邮箱（给多个目标发邮件只需创建一个列表，将邮箱放在列表中即可） args.subject 是主题 args.contents 是邮箱正文 args.attachments 是附件(传入文件路径)","link":"/2019/08/26/短信服务与邮件服务/"},{"title":"浅谈【少儿编程行业当前阶段】","text":"浅谈【少儿编程行业当前是品质为王的阶段】 少儿编程行业友商林立，大家竞争的究竟是什么？当前处于什么阶段？ 我的观点如下：竞争的领域包括品质、价格、品牌、口碑、服务、运营。市场的不同阶段（早期、中期、成熟期）竞争领域的排序不同。当前处于市场中期，对品质的要求排第一。 （下文中影响因素的排序是指对整个产品的影响比例，假设所提的影响因素的影响力之和为100%） (不是本人只能部分观看) ……………","link":"/2021/04/09/浅谈【少儿编程行业当前阶段】/"},{"title":"浅谈【站在孩子的角度，课程的乐趣有几层？】","text":"本周在磨课、与伙伴讨论课程制作标准的过程中突然意识到这个问题：站在孩子的角度，课程的乐趣有几层。想明白这个问题能让课程设计伙伴心中有杆隐形“秤”——利于案例选取、知识讲解、过程设计、迁移拓展等环节的优化。 我觉得总体上可以分为四层：1.直观感知层；2获取知识层；3.参与体验层；4创造及自我实现层。 课程第一层乐趣：直观感知层——包装 孩子对课程的第一印象、视听觉等直观感受。甚至课前通过课程名字对课程的预判。这里的关键词是“包装”。 常用手段有：加故事背景、丰富知识点外化展示效果、相关知识在素质层面的拓展等。 ​ (不是本人只能看部分…..） 举例：课程《十以内加减法》的四层乐趣 第一层乐趣——包装： 加故事背景，外星人来袭，组织战斗机群去战斗，驾驶员需要精确了解自己战斗机的子弹数。原来飞机上的子弹数+加上新加载的子弹数=？ 这是孩子需要完成的任务。 (不是本人只能看部分…..） 希望对课程四层乐趣的理解能够利于案例选取、知识讲解、过程设计、迁移拓展等环节的优化。","link":"/2021/05/09/浅谈【站在孩子的角度，课程的乐趣有几层？】/"},{"title":"少儿编程为什么学，怎么学","text":"今天无意当中看到了我弟弟的卷子中既然出现了编程题，勾起了我的兴趣并对这个行业并做了一些调查，有了一种转行做少儿编程的冲动 自从孩子上了初中，孩子妈就开始盯着各种真假难辨的中考、高考新政传言。当她从铺天盖地的少儿编程广告里获悉，编程将纳入中考，高考范围，并且2018年高考，多个省份的数学卷甚至都出现了编程题时，就变得异常兴奋。我这个写了近20年代码，家庭地位本来十分低下的资深码农，一夜之间变身“宝藏男孩”。 中考、高考政策一天一个新花样，谁也不知道将来要不要考编程。不过2018年，江苏省、天津市、北京市等省份高考数学卷出现了编程题倒是真的。所以，即使一直反对少儿学编程，我也不得不认真思考 少儿学编程有没有必要？ 花多长时间学编程上，才不会影响主课学习时间？ 怎么教，才能避免借口学编程，实际玩游戏？ 少儿学编程有没有必要 ​ 编程语言也是一种语言，只不过打交道的是电脑而已。 ​ 学语言，一般都会有两个很深的体会。 ​ 有使用环境，学起来快，比如语文；没有使用环境，学起来痛苦，比如英语。 ​ 会用和用得好完全是两回事。《新华字典》背得滚瓜烂熟也不一定能写出好文章。 ​ 学编程和学别的语言一样，只学不用，不仅学得很辛苦，如果不用，还忘得快。少儿学编程，即使死记硬背把语法都记住了，生活中没有使用的场景，渐渐也会都忘光。这也是我之前一直反对少儿学编程的主要原因。 ​ 但学编程又和学别的语言不完全一样。能不能写出好作文，更多是看天赋：情感细腻、想象力丰富，有同理心……；但能不能写出好程序，却是可以培训和锻炼的。 程序的核心是算法，算法的本质是数学。 ​ 就像2018年各省高考数学卷里的编程题，表面上看是编程题，实际上考察的还是数学上的逻辑思维能力。所以，少儿学编程，关键要看学的是什么。 ​ 死记硬背语法，或者拖拽几个小方块，快速做出一个小游戏。开始兴致盎然，但兴头一过，用不了多久就忘得差不多了。 ​ 但如果能学的是编程里最有价值的东西，数学逻辑和解决问题的思维方式。不仅高考用得着，以后工作、生活中都用得着。 花多长时间学编程 ​ 孩子学习压力已经够重了，即使学编程有好处。但是花费大量的时间，性价比高吗？会不会得不偿失，反而影响了主课的学习？ 这种担忧的本质是：主课学习和编程学习是对立的。 ​ 但假设我们学习编程的方式是： ​ 每周大约30分钟。围绕的是一个数学或者逻辑思维问题。 ​ 20分钟用来思考、讨论、写写画画，电脑都不需要打开。 ​ 10分钟用来教可以用来解决这个问题的编程知识，程序只是一个工具，帮助我们提高解决问题的效率。 ​ 不能说完全消除了两者的对立，但至少每周花的时间不多，大多数时间是花在逻辑思考上，顺便学会了编程。 怎么教，不会让学编程变成玩游戏 ​ 80后家长应该都记得“小霸王”学习机，当年几乎每个小孩人手一台。号称也是用来学电脑，学编程的。但实际上，差不多都用来玩游戏了。“魂斗罗”、“坦克大战”，……是我们最美好的童年回忆之一，甚至连“之一”都没有。 怎么让学编程不成为玩游戏的借口，说实话是一个巨大的挑战。我的实践经验是： ​ ● 每周的题目足够有趣。找到足够多有趣，能引发孩子思考，又能由易到难把编程语言知识点成体系串联起来的题目，是最大的挑战。 ​ ● 父母的参与。这些题目都是不借助电脑也能做的，前20分钟父母一起参与讨论，思考，不仅能让孩子更投入，也能增进和孩子的感情。这也是我把公众号取名“和孩子一起学Python”的初衷。 ​ ● 尽可能减少电脑操作的时间。思路理顺之后，需要电脑操作的时间不多，完全可以控制孩子用电脑的时间，像我家小朋友是个小近视，每周用电脑的时间要求她不能超过1个小时。","link":"/2019/10/21/少儿编程为什么学，怎么学/"},{"title":"少儿编程教研必备","text":"个人认为做少儿编程教研需要具备的思考维度第一点是懂编程,有不错的编程基本功，这一点很好理解。少儿编程教育虽然不是为了把孩子培养成小程序员，但毕竟要对孩子进行编程启蒙，教会孩子一些基础的编程概念和算法知识。如果有不错的编程基本功，才可以更容易的进行结合设计。 \\第二点懂教育或者懂孩子**。这一点看上去很宽泛，简单来说，就是能够用孩子能听懂的语言教会孩子编程，同时让孩子爱上编程。自己会编程，和教会别人编程（尤其是教会小学生编程），是很不一样的能力。前者要求自己逻辑能力好，聪明，而后者，则要求你抛弃自己“编程专家”的视角，切换到一个初学者视角，从孩子不太聪明的视角去思考，感受他的困惑点，然后把这些卡点掰扯清楚。另外更重要的是，要能够把自己对编程的热情传递给孩子，恰当地鼓励孩子，让孩子保持对编程的兴趣，这比教会孩子多少编程知识都要重要。 \\第三点是产品/数据素养**********可能很多人好奇为什么会有这一点，其实这点更偏向于附加要求。现在很多少儿编程课都是在线课程，而且都在一步步产品化。以某某机构为例，课程是剧情+游戏闯关的形式，每个关卡挑战都可以理解成是一个小产品demo，如果有产品方面的经验或者基本素养，能够关注到用户体验，在实际设计课程时，在关卡的玩法策划，视觉审美、孩子的操作体验上都能提出很好的建议。同时，评判一节在线教育课程，除了由有经验的教研主观判断外，更多地会结合学生学习行为数据去分析。如果有数据分析方面的经验或方法论，有利于工作时更客观地从数据维度去评价一个课程的好坏，分析出课程哪里有问题，并提出如何修改的意见。 说完岗位的能力模型，我们再根据这个反推，如果你是一名求职者可以做哪些准备。下面根据候选人的不同背景情况，具体分析。 第一种情况，有编程基础但没有教育基础，如程序员、计算机相关专业的毕业生等。 对于这种情况，编程能力一般不会是求职障碍，怎么能够具有初学者视角，有一些基本的教育认知，是比较大的挑战。怎么克服自己已经固化的“编程专家”视角，最好的莫过于亲自当几次老师试试。如果身边有适龄的小朋友，可以尝试着教几节入门编程课，在这几节课的过程中，你就会不断地去琢磨怎么讲得更容易让小朋友听懂，而孩子在听课过程中那稚嫩又困惑的表情，又会一直提醒你，不要从自己的角度去讲解。这种实践，对你切换视角是有很直接的帮助的。 怎么才能知道自己有没有初学者视角呢？这里可以做个最简单的小测试，问问自己怎么给一个三年级的小朋友解释清楚变量这个概念，如果你还在纠结“存储空间”“赋值”这些术语，说明你还没有转变思路，如果你开始用“一个盒子“或生活中的容器来打比喻，说明已经在转变固有思路了。 第二种情况，有教育背景但没有编程基础，如其他学科的教研/老师，教育师范生等。 这种情况的优势是对于教育、孩子特点已经有一定的认知，面临的卡点也很明显，自己对于编程无基础无认知，何谈把它教给学生。但其实这种背景的候选人，如果准备得当，反而能够获得一个大家容易忽视的优势。这时候，开始自学编程是必备准备，候选人以一个初学者的身份，能够更好地了解初学者学编程碰到的一些困惑，把这些困惑、卡点详细地记录下来，仔细地思考编程为什么难，这对于后期的教研工作帮助很大，这段经历反而变成一种优势。 另外，自己在教学工作中，一定要多思考多输入，去思考学生的发展特点，去接触不同类型的学生，让教学真正变成自己的长处，而不是对着已经准备好的教学大纲照本宣科，那这种“老师”基本不具备教育背景方面的优势。 第三种，有产品或数据方面的工作经历，但基本无编程或教育基础。如，产品经理或需要数据分析的岗位。 这种情况在准备时可以扬长避短，在面试时强调自己产品/数据分析方面的优势，并思考如何应用在教研工作中，毕竟这种背景的候选人在教研或师资队伍里是比较少的，可以作为团队一个很好的补充。 同时，也可以参照上面两种情况，针对性地做一些准备。最好可以自己一边学习编程，一边把学到的知识教给小朋友，实践“费曼学习法”。 第四种，背景完全不沾边，如不相关专业的应届毕业生，或者其他行业想转行的人。 这种背景，老实讲在面试时肯定是不占优势的，所以需要做的准备更多，但落选的概率也不小。除了前面提到的几点工作可以准备外，还可以考虑曲线救国，一般教育公司除了教研岗外，还有授课教师岗或者辅导老师岗，其中有一些老师岗位，要求没有那么高，候选人可以根据自身条件，看看是不是先从老师岗位入手，积累一些教学经验，再择机转岗教研，这也是一个思路。","link":"/2019/12/01/少儿编程教研必备/"}],"tags":[{"name":"Excel","slug":"Excel","link":"/tags/Excel/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"环境配置","slug":"环境配置","link":"/tags/环境配置/"},{"name":"MongoDB数据库","slug":"MongoDB数据库","link":"/tags/MongoDB数据库/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"部署","slug":"部署","link":"/tags/部署/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"Bug","slug":"Bug","link":"/tags/Bug/"},{"name":"mysql+ubuntu","slug":"mysql-ubuntu","link":"/tags/mysql-ubuntu/"},{"name":"爬虫","slug":"爬虫","link":"/tags/爬虫/"},{"name":"scrapy","slug":"scrapy","link":"/tags/scrapy/"},{"name":"scrapy-1-快速入门","slug":"scrapy-1-快速入门","link":"/tags/scrapy-1-快速入门/"},{"name":"scrapy-3-数据处理","slug":"scrapy-3-数据处理","link":"/tags/scrapy-3-数据处理/"},{"name":"scrapy-2-数据解析","slug":"scrapy-2-数据解析","link":"/tags/scrapy-2-数据解析/"},{"name":"scrapy-4-链接提取器","slug":"scrapy-4-链接提取器","link":"/tags/scrapy-4-链接提取器/"},{"name":"scrapy-5-文件图片下载","slug":"scrapy-5-文件图片下载","link":"/tags/scrapy-5-文件图片下载/"},{"name":"selenium","slug":"selenium","link":"/tags/selenium/"},{"name":"爬虫+selenium","slug":"爬虫-selenium","link":"/tags/爬虫-selenium/"},{"name":"浅谈少儿编程","slug":"浅谈少儿编程","link":"/tags/浅谈少儿编程/"},{"name":"python，反爬","slug":"python，反爬","link":"/tags/python，反爬/"},{"name":"短信与邮件服务","slug":"短信与邮件服务","link":"/tags/短信与邮件服务/"}],"categories":[{"name":"Excel","slug":"Excel","link":"/categories/Excel/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Excel-openpyxl-增删改查","slug":"Excel/Excel-openpyxl-增删改查","link":"/categories/Excel/Excel-openpyxl-增删改查/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"环境配置","slug":"环境配置","link":"/categories/环境配置/"},{"name":"Excel-openpyxl-创建和读取","slug":"Excel/Excel-openpyxl-创建和读取","link":"/categories/Excel/Excel-openpyxl-创建和读取/"},{"name":"Django","slug":"Django","link":"/categories/Django/"},{"name":"Excel-openpyxl--数据梳理","slug":"Excel/Excel-openpyxl-数据梳理","link":"/categories/Excel/Excel-openpyxl-数据梳理/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"bug","slug":"bug","link":"/categories/bug/"},{"name":"爬虫","slug":"爬虫","link":"/categories/爬虫/"},{"name":"scrapy-2-数据解析","slug":"爬虫/scrapy-2-数据解析","link":"/categories/爬虫/scrapy-2-数据解析/"},{"name":"scrapy-1-快速入门","slug":"爬虫/scrapy-1-快速入门","link":"/categories/爬虫/scrapy-1-快速入门/"},{"name":"scrapy-4-链接提取器","slug":"爬虫/scrapy-4-链接提取器","link":"/categories/爬虫/scrapy-4-链接提取器/"},{"name":"scrapy-5-文件图片下载","slug":"爬虫/scrapy-5-文件图片下载","link":"/categories/爬虫/scrapy-5-文件图片下载/"},{"name":"scrapy-3-数据处理","slug":"爬虫/scrapy-3-数据处理","link":"/categories/爬虫/scrapy-3-数据处理/"},{"name":"Selenium","slug":"Selenium","link":"/categories/Selenium/"},{"name":"浅谈少儿编程","slug":"浅谈少儿编程","link":"/categories/浅谈少儿编程/"},{"name":"反爬","slug":"爬虫/反爬","link":"/categories/爬虫/反爬/"},{"name":"smscode","slug":"smscode","link":"/categories/smscode/"},{"name":"爬虫+selenium","slug":"Selenium/爬虫-selenium","link":"/categories/Selenium/爬虫-selenium/"},{"name":"sendemail","slug":"smscode/sendemail","link":"/categories/smscode/sendemail/"},{"name":"技术","slug":"技术","link":"/categories/技术/"},{"name":"少儿编程三问","slug":"浅谈少儿编程/少儿编程三问","link":"/categories/浅谈少儿编程/少儿编程三问/"},{"name":"教研必备","slug":"浅谈少儿编程/教研必备","link":"/categories/浅谈少儿编程/教研必备/"},{"name":"浅谈【少儿编程行业当前阶段】","slug":"浅谈少儿编程/浅谈【少儿编程行业当前阶段】","link":"/categories/浅谈少儿编程/浅谈【少儿编程行业当前阶段】/"},{"name":"感-《终身幼儿园》","slug":"浅谈少儿编程/感-《终身幼儿园》","link":"/categories/浅谈少儿编程/感-《终身幼儿园》/"},{"name":"浅谈【站在孩子的角度，课程的乐趣有几层？】","slug":"浅谈少儿编程/浅谈【站在孩子的角度，课程的乐趣有几层？】","link":"/categories/浅谈少儿编程/浅谈【站在孩子的角度，课程的乐趣有几层？】/"}]}